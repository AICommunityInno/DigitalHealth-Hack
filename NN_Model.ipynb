{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, InputLayer\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy\n",
    "\n",
    "from os.path import join, basename, exists\n",
    "from os import makedirs, listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_target_train(data, reduce_classes=False):\n",
    "    diagnoses = data['Код_диагноза'].copy()\n",
    "    \n",
    "    if reduce_classes:\n",
    "        pop_diagnoses = set(utils.get_most_popular_diagnoses(diagnoses, percent=.80))\n",
    "        most_pop_diagnose = scipy.stats.mode(diagnoses)[0][0]\n",
    "    else:\n",
    "        pop_diagnoses = set(diagnoses)\n",
    "        most_pop_diagnose = scipy.stats.mode(diagnoses)[0][0]\n",
    "    \n",
    "    diagnoses = diagnoses.apply(\n",
    "        lambda diag: diag if diag in pop_diagnoses else most_pop_diagnose\n",
    "    )\n",
    "    \n",
    "    return diagnoses, pop_diagnoses, most_pop_diagnose\n",
    "\n",
    "def preproc_target_test(data, pop_diagnoses, most_pop_diagnose):\n",
    "    diagnoses = data['Код_диагноза'].copy()\n",
    "    \n",
    "    diagnoses = diagnoses.apply(\n",
    "        lambda diag: diag if diag in pop_diagnoses else most_pop_diagnose\n",
    "    )\n",
    "    \n",
    "    return diagnoses, pop_diagnoses, most_pop_diagnose\n",
    "\n",
    "def join_topics(data, topics):\n",
    "    data = data.copy()\n",
    "    \n",
    "    topics_df = pd.DataFrame(dict(zip(\n",
    "        ['topic' + str(i) for i in range(topics.shape[1])],\n",
    "        [topics[:, i] for i in range(topics.shape[1])])))\n",
    "    topics_df['Id_Записи'] = data['Id_Записи']\n",
    "\n",
    "    data = data.join(topics_df, on='Id_Записи', rsuffix='_topics', how='outer')\n",
    "    data = data.drop(columns=['Id_Записи_topics'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class DoctorsPopularityTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        doctors = x.fillna('sss')\n",
    "        doctors_voc, counts = np.unique(doctors, return_counts=True)\n",
    "        self.pop_doctor = doctors_voc[np.argsort(counts)[::-1][0]]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = x.fillna('sss')\n",
    "        x[x == 'sss'] = self.pop_doctor\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GenderTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        x[x == 1] = 0\n",
    "        x[x == 2] = 1\n",
    "        \n",
    "        return np.expand_dims(x, axis=1)\n",
    "    \n",
    "class AgeTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return np.expand_dims(x, axis=1)\n",
    "    \n",
    "class TopicsTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, batch_size, epochs, tb_log_dir, model_path, validation_split=0.3):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.tb_log_dir = tb_log_dir\n",
    "        self.model_path = model_path\n",
    "        self.validation_split = validation_split\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        self.classes = np.unique(y)\n",
    "        self.classes_voc = dict(zip(self.classes, range(self.classes.shape[0])))\n",
    "        self.voc_classes = dict(zip(range(self.classes.shape[0]), self.classes))\n",
    "        y_proc = np.zeros((y.shape[0], self.classes.shape[0]), dtype=np.float32)\n",
    "        for i, yc in enumerate(y):\n",
    "            y_proc[i, self.classes_voc[yc]] = 1.\n",
    "        \n",
    "        self.model = Sequential([\n",
    "            InputLayer(input_shape=(x.shape[1],)),\n",
    "            Dense(2048, activation='sigmoid'),\n",
    "            Dense(2048, activation='sigmoid'),\n",
    "            Dense(self.classes.shape[0], activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        optim = RMSprop(lr=1e-4, decay=1e-6)\n",
    "        self.model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model.fit(x, y_proc,\n",
    "                  batch_size=self.batch_size, epochs=self.epochs,\n",
    "                  callbacks=[\n",
    "                      TensorBoard(log_dir=self.tb_log_dir, batch_size=self.batch_size),\n",
    "                      ModelCheckpoint(filepath=self.model_path, monitor='acc', period=5)\n",
    "                  ],\n",
    "                  validation_split=self.validation_split)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, x):\n",
    "        pred = self.model.predict(x)\n",
    "        max_idxs = np.argmax(pred, axis=1)\n",
    "        \n",
    "        return np.array(list(map(lambda max_idx: self.voc_classes[max_idx], max_idxs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_validation = False\n",
    "validation_split = 0.3\n",
    "\n",
    "experiment_dir = 'simple_models'\n",
    "model_name = 'nn_3dense' + str(utils.get_next_model_id(experiment_dir))\n",
    "tb_log_dir = join(experiment_dir, 'log', model_name)\n",
    "models_dir = join(experiment_dir, 'models')\n",
    "\n",
    "if no_validation:\n",
    "    model_path = utils.get_model_fname_pattern(models_dir, model_name, no_validation=True)\n",
    "    validation_split = 0.0\n",
    "else:\n",
    "    model_path = utils.get_model_fname_pattern(models_dir, model_name, no_validation=False)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('complaints_pipe', Pipeline([\n",
    "                ('complaint_selector', ItemSelector(key='Жалобы (unigramm)')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,1), min_df=10, stop_words=stopwords.words('russian')))\n",
    "            ])),\n",
    "            ('complaints_n_pipe', Pipeline([\n",
    "                ('complaint_n_selector', ItemSelector(key='Жалобы (ngramm)')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,1), min_df=1, stop_words=stopwords.words('russian')))\n",
    "            ])),\n",
    "            ('doctor_pipe', Pipeline([\n",
    "                ('doctor_selector', ItemSelector(key='Врач')),\n",
    "                ('doc_pop', DoctorsPopularityTransformator()),\n",
    "                ('count_vect', CountVectorizer())\n",
    "            ])),\n",
    "            ('gender_pipe', Pipeline([\n",
    "                ('gender_selector', ItemSelector(key='Пол')),\n",
    "                ('gender_transform', GenderTransformator())\n",
    "            ])),\n",
    "            ('age_pipe', Pipeline([\n",
    "                ('age_selector', ItemSelector(key='Возраст')),\n",
    "                ('age_transformator', AgeTransformator())\n",
    "            ])),\n",
    "            ('topics_pipe', Pipeline([\n",
    "                ('topic_selector', ItemSelector(key=['topic' + str(i) for i in range(355)])),\n",
    "                ('topics_transform', TopicsTransformator())\n",
    "            ]))\n",
    "        ]\n",
    "    )),\n",
    "    ('clf', NNModel(batch_size=128, epochs=200, tb_log_dir=tb_log_dir,\n",
    "                    model_path=model_path, validation_split=validation_split))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = utils.load_data('data/train_data_complaints_repeats_doctors.csv')\n",
    "train_topics = np.load('data/topics_train_ngramm.npy')\n",
    "train = join_topics(train, train_topics)\n",
    "# train, valid = train_test_split(train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train_y, pop_diagnoses, most_pop_diagnose = preproc_target_train(train, reduce_classes=False)\n",
    "# valid_y, _, _ = preproc_target_test(valid, pop_diagnoses, most_pop_diagnose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2302,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43383 samples, validate on 18593 samples\n",
      "Epoch 1/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 5.6862 - acc: 0.0486 - val_loss: 6.1860 - val_acc: 0.0457\n",
      "Epoch 2/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 5.4604 - acc: 0.0716 - val_loss: 6.0422 - val_acc: 0.0769\n",
      "Epoch 3/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 5.2197 - acc: 0.0949 - val_loss: 5.8356 - val_acc: 0.0863\n",
      "Epoch 4/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 4.9356 - acc: 0.1150 - val_loss: 5.6096 - val_acc: 0.1228\n",
      "Epoch 5/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 4.7070 - acc: 0.1615 - val_loss: 5.4025 - val_acc: 0.1518\n",
      "Epoch 6/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 4.4800 - acc: 0.1818 - val_loss: 5.2101 - val_acc: 0.1676\n",
      "Epoch 7/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 4.2659 - acc: 0.1987 - val_loss: 5.0850 - val_acc: 0.1890\n",
      "Epoch 8/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 4.0806 - acc: 0.2134 - val_loss: 4.9290 - val_acc: 0.1987\n",
      "Epoch 9/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 3.9314 - acc: 0.2248 - val_loss: 4.8413 - val_acc: 0.2040\n",
      "Epoch 10/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 3.8127 - acc: 0.2332 - val_loss: 4.7391 - val_acc: 0.2059\n",
      "Epoch 11/200\n",
      "43383/43383 [==============================] - 7s 173us/step - loss: 3.7185 - acc: 0.2403 - val_loss: 4.6328 - val_acc: 0.2178\n",
      "Epoch 12/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 3.6410 - acc: 0.2480 - val_loss: 4.5978 - val_acc: 0.2218\n",
      "Epoch 13/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 3.5779 - acc: 0.2529 - val_loss: 4.5660 - val_acc: 0.2219\n",
      "Epoch 14/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 3.5228 - acc: 0.2560 - val_loss: 4.4683 - val_acc: 0.2262\n",
      "Epoch 15/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 3.4772 - acc: 0.2593 - val_loss: 4.4458 - val_acc: 0.2299\n",
      "Epoch 16/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 3.4356 - acc: 0.2615 - val_loss: 4.4304 - val_acc: 0.2284\n",
      "Epoch 17/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 3.3993 - acc: 0.2651 - val_loss: 4.4031 - val_acc: 0.2315\n",
      "Epoch 18/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 3.3662 - acc: 0.2673 - val_loss: 4.3711 - val_acc: 0.2358\n",
      "Epoch 19/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 3.3342 - acc: 0.2705 - val_loss: 4.3321 - val_acc: 0.2360\n",
      "Epoch 20/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 3.3056 - acc: 0.2736 - val_loss: 4.3006 - val_acc: 0.2389\n",
      "Epoch 21/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.2778 - acc: 0.2775 - val_loss: 4.2708 - val_acc: 0.2451\n",
      "Epoch 22/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 3.2519 - acc: 0.2801 - val_loss: 4.2522 - val_acc: 0.2438\n",
      "Epoch 23/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 3.2282 - acc: 0.2820 - val_loss: 4.2031 - val_acc: 0.2467\n",
      "Epoch 24/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 3.2053 - acc: 0.2855 - val_loss: 4.2207 - val_acc: 0.2469\n",
      "Epoch 25/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 3.1852 - acc: 0.2871 - val_loss: 4.2029 - val_acc: 0.2472\n",
      "Epoch 26/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 3.1666 - acc: 0.2905 - val_loss: 4.1817 - val_acc: 0.2517\n",
      "Epoch 27/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 3.1486 - acc: 0.2918 - val_loss: 4.1700 - val_acc: 0.2534\n",
      "Epoch 28/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 3.1306 - acc: 0.2941 - val_loss: 4.1328 - val_acc: 0.2589\n",
      "Epoch 29/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 3.1136 - acc: 0.2962 - val_loss: 4.1553 - val_acc: 0.2571\n",
      "Epoch 30/200\n",
      "43383/43383 [==============================] - 8s 183us/step - loss: 3.0970 - acc: 0.2986 - val_loss: 4.1396 - val_acc: 0.2579\n",
      "Epoch 31/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 3.0836 - acc: 0.3012 - val_loss: 4.1036 - val_acc: 0.2630\n",
      "Epoch 32/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 3.0692 - acc: 0.3027 - val_loss: 4.0882 - val_acc: 0.2646\n",
      "Epoch 33/200\n",
      "43383/43383 [==============================] - 7s 173us/step - loss: 3.0538 - acc: 0.3047 - val_loss: 4.0716 - val_acc: 0.2631\n",
      "Epoch 34/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 3.0415 - acc: 0.3068 - val_loss: 4.0755 - val_acc: 0.2638\n",
      "Epoch 35/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.0294 - acc: 0.3090 - val_loss: 4.0788 - val_acc: 0.2669\n",
      "Epoch 36/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 3.0180 - acc: 0.3099 - val_loss: 4.0777 - val_acc: 0.2668\n",
      "Epoch 37/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 3.0058 - acc: 0.3124 - val_loss: 4.0634 - val_acc: 0.2698\n",
      "Epoch 38/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.9953 - acc: 0.3145 - val_loss: 4.0596 - val_acc: 0.2728\n",
      "Epoch 39/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.9848 - acc: 0.3165 - val_loss: 4.0581 - val_acc: 0.2708\n",
      "Epoch 40/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.9754 - acc: 0.3175 - val_loss: 4.0427 - val_acc: 0.2719\n",
      "Epoch 41/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.9652 - acc: 0.3187 - val_loss: 3.9993 - val_acc: 0.2717\n",
      "Epoch 42/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.9544 - acc: 0.3195 - val_loss: 4.0318 - val_acc: 0.2757\n",
      "Epoch 43/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.9452 - acc: 0.3209 - val_loss: 4.0175 - val_acc: 0.2777\n",
      "Epoch 44/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.9368 - acc: 0.3227 - val_loss: 4.0123 - val_acc: 0.2776\n",
      "Epoch 45/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.9266 - acc: 0.3248 - val_loss: 4.0053 - val_acc: 0.2763\n",
      "Epoch 46/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.9122 - acc: 0.3248 - val_loss: 3.9757 - val_acc: 0.2769\n",
      "Epoch 47/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.9052 - acc: 0.3266 - val_loss: 3.9902 - val_acc: 0.2791\n",
      "Epoch 48/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.8972 - acc: 0.3255 - val_loss: 4.0037 - val_acc: 0.2783\n",
      "Epoch 49/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.8909 - acc: 0.3276 - val_loss: 3.9879 - val_acc: 0.2828\n",
      "Epoch 50/200\n",
      "43383/43383 [==============================] - 8s 181us/step - loss: 2.8833 - acc: 0.3286 - val_loss: 3.9881 - val_acc: 0.2823\n",
      "Epoch 51/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.8783 - acc: 0.3293 - val_loss: 3.9584 - val_acc: 0.2828\n",
      "Epoch 52/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.8724 - acc: 0.3298 - val_loss: 4.0167 - val_acc: 0.2830\n",
      "Epoch 53/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.8632 - acc: 0.3301 - val_loss: 3.9753 - val_acc: 0.2840\n",
      "Epoch 54/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.8525 - acc: 0.3315 - val_loss: 3.9571 - val_acc: 0.2846\n",
      "Epoch 55/200\n",
      "43383/43383 [==============================] - 7s 171us/step - loss: 2.8452 - acc: 0.3322 - val_loss: 3.9795 - val_acc: 0.2835\n",
      "Epoch 56/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.8414 - acc: 0.3334 - val_loss: 3.9299 - val_acc: 0.2841\n",
      "Epoch 57/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.8379 - acc: 0.3334 - val_loss: 3.9341 - val_acc: 0.2832\n",
      "Epoch 58/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.8319 - acc: 0.3346 - val_loss: 3.9930 - val_acc: 0.2841\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.8231 - acc: 0.3356 - val_loss: 3.9413 - val_acc: 0.2859\n",
      "Epoch 60/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.8191 - acc: 0.3356 - val_loss: 3.9315 - val_acc: 0.2850\n",
      "Epoch 61/200\n",
      "43383/43383 [==============================] - 8s 181us/step - loss: 2.8154 - acc: 0.3373 - val_loss: 3.9596 - val_acc: 0.2876\n",
      "Epoch 62/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.8099 - acc: 0.3367 - val_loss: 3.9271 - val_acc: 0.2874\n",
      "Epoch 63/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.8017 - acc: 0.3378 - val_loss: 3.9443 - val_acc: 0.2869\n",
      "Epoch 64/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.7981 - acc: 0.3385 - val_loss: 3.9692 - val_acc: 0.2882\n",
      "Epoch 65/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7962 - acc: 0.3392 - val_loss: 3.9518 - val_acc: 0.2872\n",
      "Epoch 66/200\n",
      "43383/43383 [==============================] - 8s 181us/step - loss: 2.7900 - acc: 0.3399 - val_loss: 3.9606 - val_acc: 0.2884\n",
      "Epoch 67/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.7849 - acc: 0.3406 - val_loss: 3.9608 - val_acc: 0.2893\n",
      "Epoch 68/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.7805 - acc: 0.3423 - val_loss: 3.9446 - val_acc: 0.2906\n",
      "Epoch 69/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.7726 - acc: 0.3415 - val_loss: 3.9020 - val_acc: 0.2911\n",
      "Epoch 70/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.7638 - acc: 0.3418 - val_loss: 3.9115 - val_acc: 0.2908\n",
      "Epoch 71/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7625 - acc: 0.3421 - val_loss: 3.9348 - val_acc: 0.2916\n",
      "Epoch 72/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.7610 - acc: 0.3445 - val_loss: 3.9351 - val_acc: 0.2910\n",
      "Epoch 73/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.7570 - acc: 0.3455 - val_loss: 3.9569 - val_acc: 0.2916\n",
      "Epoch 74/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.7551 - acc: 0.3450 - val_loss: 3.9363 - val_acc: 0.2919\n",
      "Epoch 75/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.7491 - acc: 0.3465 - val_loss: 3.9332 - val_acc: 0.2943\n",
      "Epoch 76/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.7413 - acc: 0.3458 - val_loss: 3.9095 - val_acc: 0.2935\n",
      "Epoch 77/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.7369 - acc: 0.3468 - val_loss: 3.8816 - val_acc: 0.2947\n",
      "Epoch 78/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.7353 - acc: 0.3482 - val_loss: 3.9263 - val_acc: 0.2936\n",
      "Epoch 79/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7265 - acc: 0.3490 - val_loss: 3.9147 - val_acc: 0.2951\n",
      "Epoch 80/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.7245 - acc: 0.3476 - val_loss: 3.9179 - val_acc: 0.2925\n",
      "Epoch 81/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.7139 - acc: 0.3495 - val_loss: 3.9082 - val_acc: 0.2951\n",
      "Epoch 82/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.7100 - acc: 0.3500 - val_loss: 3.9205 - val_acc: 0.2924\n",
      "Epoch 83/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.7104 - acc: 0.3501 - val_loss: 3.9345 - val_acc: 0.2949\n",
      "Epoch 84/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.7115 - acc: 0.3504 - val_loss: 3.9482 - val_acc: 0.2936\n",
      "Epoch 85/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.7071 - acc: 0.3521 - val_loss: 3.9106 - val_acc: 0.2982\n",
      "Epoch 86/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.7066 - acc: 0.3508 - val_loss: 3.9202 - val_acc: 0.2969\n",
      "Epoch 87/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7045 - acc: 0.3529 - val_loss: 3.9545 - val_acc: 0.2937\n",
      "Epoch 88/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.7034 - acc: 0.3526 - val_loss: 3.9811 - val_acc: 0.2947\n",
      "Epoch 89/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6968 - acc: 0.3523 - val_loss: 3.9518 - val_acc: 0.2962\n",
      "Epoch 90/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.6931 - acc: 0.3546 - val_loss: 3.9360 - val_acc: 0.2977\n",
      "Epoch 91/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6878 - acc: 0.3553 - val_loss: 3.9486 - val_acc: 0.2967\n",
      "Epoch 92/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6815 - acc: 0.3573 - val_loss: 3.9324 - val_acc: 0.2978\n",
      "Epoch 93/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.6796 - acc: 0.3547 - val_loss: 3.9316 - val_acc: 0.2955\n",
      "Epoch 94/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6714 - acc: 0.3566 - val_loss: 3.9033 - val_acc: 0.2975\n",
      "Epoch 95/200\n",
      "43383/43383 [==============================] - 8s 181us/step - loss: 2.6718 - acc: 0.3570 - val_loss: 3.9462 - val_acc: 0.2972\n",
      "Epoch 96/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.6717 - acc: 0.3579 - val_loss: 3.9344 - val_acc: 0.2992\n",
      "Epoch 97/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6726 - acc: 0.3576 - val_loss: 3.9295 - val_acc: 0.2982\n",
      "Epoch 98/200\n",
      "43383/43383 [==============================] - 7s 171us/step - loss: 2.6632 - acc: 0.3587 - val_loss: 3.8909 - val_acc: 0.2966\n",
      "Epoch 99/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6597 - acc: 0.3591 - val_loss: 3.8983 - val_acc: 0.2992\n",
      "Epoch 100/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.6622 - acc: 0.3591 - val_loss: 3.9272 - val_acc: 0.2985\n",
      "Epoch 101/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6575 - acc: 0.3608 - val_loss: 3.9294 - val_acc: 0.3023\n",
      "Epoch 102/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6532 - acc: 0.3610 - val_loss: 3.8839 - val_acc: 0.3000\n",
      "Epoch 103/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.6504 - acc: 0.3621 - val_loss: 3.9354 - val_acc: 0.2993\n",
      "Epoch 104/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6532 - acc: 0.3630 - val_loss: 3.9388 - val_acc: 0.2993\n",
      "Epoch 105/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.6424 - acc: 0.3624 - val_loss: 3.9106 - val_acc: 0.2965\n",
      "Epoch 106/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6319 - acc: 0.3629 - val_loss: 3.9019 - val_acc: 0.3002\n",
      "Epoch 107/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6216 - acc: 0.3637 - val_loss: 3.9107 - val_acc: 0.2983\n",
      "Epoch 108/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6306 - acc: 0.3638 - val_loss: 3.9611 - val_acc: 0.2993\n",
      "Epoch 109/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.6368 - acc: 0.3644 - val_loss: 3.9880 - val_acc: 0.2999\n",
      "Epoch 110/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.6392 - acc: 0.3666 - val_loss: 3.9636 - val_acc: 0.3010\n",
      "Epoch 111/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.6343 - acc: 0.3649 - val_loss: 3.9478 - val_acc: 0.2990\n",
      "Epoch 112/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6200 - acc: 0.3662 - val_loss: 3.9273 - val_acc: 0.3002\n",
      "Epoch 113/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6214 - acc: 0.3665 - val_loss: 3.9531 - val_acc: 0.3026\n",
      "Epoch 114/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.6203 - acc: 0.3659 - val_loss: 3.9463 - val_acc: 0.2997\n",
      "Epoch 115/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6225 - acc: 0.3683 - val_loss: 3.9662 - val_acc: 0.3011\n",
      "Epoch 116/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.6197 - acc: 0.3685 - val_loss: 3.9517 - val_acc: 0.3028\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6132 - acc: 0.3682 - val_loss: 3.9657 - val_acc: 0.3022\n",
      "Epoch 118/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6171 - acc: 0.3693 - val_loss: 3.9990 - val_acc: 0.3032\n",
      "Epoch 119/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.6224 - acc: 0.3705 - val_loss: 3.9410 - val_acc: 0.3027\n",
      "Epoch 120/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6085 - acc: 0.3710 - val_loss: 3.9514 - val_acc: 0.3019\n",
      "Epoch 121/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.6030 - acc: 0.3715 - val_loss: 3.9468 - val_acc: 0.3023\n",
      "Epoch 122/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6050 - acc: 0.3740 - val_loss: 3.9831 - val_acc: 0.3022\n",
      "Epoch 123/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6097 - acc: 0.3723 - val_loss: 4.0050 - val_acc: 0.3022\n",
      "Epoch 124/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6079 - acc: 0.3728 - val_loss: 3.9964 - val_acc: 0.3029\n",
      "Epoch 125/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6022 - acc: 0.3742 - val_loss: 3.9879 - val_acc: 0.3022\n",
      "Epoch 126/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.5994 - acc: 0.3741 - val_loss: 3.9955 - val_acc: 0.3027\n",
      "Epoch 127/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5993 - acc: 0.3750 - val_loss: 3.9326 - val_acc: 0.3021\n",
      "Epoch 128/200\n",
      "43383/43383 [==============================] - 7s 172us/step - loss: 2.5611 - acc: 0.3758 - val_loss: 3.8654 - val_acc: 0.3016\n",
      "Epoch 129/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.5645 - acc: 0.3759 - val_loss: 3.9174 - val_acc: 0.3030\n",
      "Epoch 130/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.5647 - acc: 0.3767 - val_loss: 3.9152 - val_acc: 0.3015\n",
      "Epoch 131/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5693 - acc: 0.3777 - val_loss: 3.9636 - val_acc: 0.3027\n",
      "Epoch 132/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.5787 - acc: 0.3774 - val_loss: 3.9895 - val_acc: 0.3006\n",
      "Epoch 133/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5787 - acc: 0.3778 - val_loss: 3.9404 - val_acc: 0.3030\n",
      "Epoch 134/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5648 - acc: 0.3779 - val_loss: 3.9616 - val_acc: 0.3026\n",
      "Epoch 135/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5669 - acc: 0.3796 - val_loss: 3.9741 - val_acc: 0.3034\n",
      "Epoch 136/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5669 - acc: 0.3776 - val_loss: 3.9824 - val_acc: 0.3021\n",
      "Epoch 137/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.5688 - acc: 0.3794 - val_loss: 3.9838 - val_acc: 0.3003\n",
      "Epoch 138/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.5622 - acc: 0.3799 - val_loss: 3.9689 - val_acc: 0.3033\n",
      "Epoch 139/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5681 - acc: 0.3796 - val_loss: 3.9941 - val_acc: 0.3059\n",
      "Epoch 140/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.5638 - acc: 0.3799 - val_loss: 3.9931 - val_acc: 0.3057\n",
      "Epoch 141/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.5695 - acc: 0.3813 - val_loss: 4.0177 - val_acc: 0.3030\n",
      "Epoch 142/200\n",
      "43136/43383 [============================>.] - ETA: 0s - loss: 2.5673 - acc: 0.3813"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f143415d9116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-c88c7f629660>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m                       \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   ],\n\u001b[0;32m---> 33\u001b[0;31m                   validation_split=self.validation_split)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1248\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1250\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipe.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A46', 'A63.0', 'B00', 'B00.1', 'B00.8', 'B02', 'B07', 'B08.1',\n",
       "        'B35.1', 'B35.3', 'B35.6', 'B36.0', 'B37', 'B37.3+', 'B97.1',\n",
       "        'B97.7', 'D17.0', 'D17.1', 'D17.2', 'D18.0', 'D23', 'D23.3',\n",
       "        'D23.5', 'D23.9', 'D24', 'D25', 'D50', 'D50.9', 'D89.8', 'E01.1',\n",
       "        'E01.8', 'E03.8', 'E03.9', 'E04.1', 'E04.2', 'E05.0', 'E06.3',\n",
       "        'E11', 'E22.1', 'E28', 'E28.1', 'E28.8', 'E28.9', 'E66.0', 'E89.0',\n",
       "        'F41.0', 'F41.2', 'F45.3', 'F95.0', 'G24', 'G24.9', 'G43.0',\n",
       "        'G44.2', 'G50.0', 'G51.0', 'G55*', 'G55.1*', 'G55.3*', 'G56.0',\n",
       "        'G56.2', 'G90', 'G90.8', 'G90.9', 'G93.4', 'H00', 'H00.0', 'H00.1',\n",
       "        'H01.0', 'H04.1', 'H10.0', 'H10.1', 'H10.2', 'H10.3', 'H10.5',\n",
       "        'H10.8', 'H11.3', 'H15.1', 'H16.1', 'H16.2', 'H20.0', 'H35.0',\n",
       "        'H35.3', 'H35.4', 'H40.1', 'H52.0', 'H52.1', 'H52.2', 'H52.4',\n",
       "        'H52.5', 'H60', 'H61.2', 'H65', 'H65.0', 'H65.1', 'H66.0', 'H66.1',\n",
       "        'H68.0', 'H68.1', 'H81.1', 'H81.2', 'H90.3', 'H93.1', 'I10', 'I11',\n",
       "        'I11.9', 'I20', 'I20.1', 'I20.8', 'I20.9', 'I25', 'I25.0', 'I25.1',\n",
       "        'I34.1', 'I47', 'I48', 'I49.8', 'I67.2', 'I67.4', 'I67.8', 'I70.2',\n",
       "        'I83', 'I83.9', 'I84.3', 'I84.4', 'I84.5', 'I86.1', 'I87.0',\n",
       "        'I87.2', 'J00', 'J01.0', 'J01.1', 'J01.8', 'J02', 'J02.9', 'J03',\n",
       "        'J03.9', 'J04.0', 'J04.1', 'J06.0', 'J06.9', 'J18', 'J18.0',\n",
       "        'J18.9', 'J20', 'J20.9', 'J30', 'J30.0', 'J30.1', 'J30.3', 'J31.0',\n",
       "        'J31.1', 'J31.2', 'J34.2', 'J35.0', 'J35.2', 'J35.3', 'J35.8',\n",
       "        'J41.0', 'J45.0', 'J45.8', 'J45.9', 'K02.1', 'K04.0', 'K05.1',\n",
       "        'K05.3', 'K21', 'K21.0', 'K21.9', 'K29.3', 'K29.5', 'K29.9', 'K30',\n",
       "        'K40', 'K40.9', 'K43', 'K46', 'K52.9', 'K58', 'K58.0', 'K80.1',\n",
       "        'K81', 'K82.8', 'K83.9', 'K86.1', 'K87*', 'L01.0', 'L02.0',\n",
       "        'L02.2', 'L02.4', 'L04.0', 'L08.0', 'L08.8', 'L20', 'L20.8', 'L21',\n",
       "        'L23', 'L23.9', 'L28.0', 'L30.0', 'L30.2', 'L30.3', 'L30.8',\n",
       "        'L30.9', 'L40.0', 'L52', 'L60.0', 'L60.3', 'L63', 'L65.0', 'L70.0',\n",
       "        'L71', 'L71.0', 'L72', 'L73.2', 'L84', 'M06.0', 'M12.5', 'M15-M19',\n",
       "        'M15.8', 'M17', 'M17.1', 'M17.4', 'M20-M25', 'M21.4', 'M22.4',\n",
       "        'M23.2', 'M41', 'M41.1', 'M42', 'M42.1', 'M47', 'M47.8', 'M50.1',\n",
       "        'M51', 'M51.1', 'M51.3', 'M51.8', 'M53.0', 'M53.1', 'M53.8',\n",
       "        'M53.9', 'M54', 'M54.1', 'M54.2', 'M54.4', 'M54.5', 'M54.6',\n",
       "        'M54.8', 'M54.9', 'M65', 'M67.4', 'M70.2', 'M71.8', 'M72.5',\n",
       "        'M75.3', 'M76.0', 'M77.1', 'M77.3', 'M77.9', 'M79.1', 'M79.2',\n",
       "        'M79.6', 'N10', 'N11', 'N20', 'N20.0', 'N20.2', 'N30', 'N30.0',\n",
       "        'N30.2', 'N34', 'N34.1', 'N34.2', 'N40', 'N41.0', 'N41.1', 'N42',\n",
       "        'N42.8', 'N46', 'N47', 'N48.1', 'N48.6', 'N48.8', 'N60.1', 'N64',\n",
       "        'N64.4', 'N70', 'N72', 'N75.0', 'N76.0', 'N76.1', 'N77.1*',\n",
       "        'N80.0', 'N83.0', 'N83.2', 'N84.0', 'N85.0', 'N86', 'N94.0',\n",
       "        'N94.9', 'N95.2', 'O04', 'R52.0', 'S06.0', 'S20.2', 'S22.3',\n",
       "        'S52.5', 'S61.0', 'S62.3', 'S80', 'S80.0', 'S83.5', 'S92.3',\n",
       "        'S92.5', 'S93.4', 'T15.1', 'Z00', 'Z00.0', 'Z00.8', 'Z01.0',\n",
       "        'Z01.4', 'Z01.8', 'Z03.3', 'Z04.8', 'Z10.0', 'Z32.0', 'Z32.1',\n",
       "        'Z34.0', 'Z34.8', 'Z35', 'Z35.8'], dtype='<U7'),\n",
       " array([  49,    1,  133,    2,   17,   76,  342,   31,   70,   54,   51,\n",
       "         112,   12,  288,    1,   22,   21,    8,   33,    9,   36,    2,\n",
       "          45,  765,   39,  300,   19,  104,   18,   27,    2,  311,   41,\n",
       "          51,  687,   10,   74,  242,    7,  197,    1,   19,    3,   48,\n",
       "          19,   16,    2,   61,    8,   48,    9,    8,  253,   70,    5,\n",
       "          41,   22,   47,    2,   42,  442,   76,  114,   31,   27,   59,\n",
       "          19,   12,    5,   22,   29,  202,   14,   37,    1,    5,   16,\n",
       "           2,    2,    1,  303,   14,   27,    1,   53,  800,   66,  123,\n",
       "           9,  252,  686,   10,   10,    6,    7,   17,    6,    7,    4,\n",
       "           7,   14,   27,  168, 2224,   96,    2,    7,   80,   71,   12,\n",
       "          16,   16,   85,    9,   18,   73,  222,    6,  233,    9,   14,\n",
       "         490,   45,   65,   79,   10,    5,    2, 1297,  839,    2,   13,\n",
       "          36,   15,   19,   19,   49,  379,   28, 3165,  222,    8,   85,\n",
       "           6,   58,    2,   79,  122,   87,  100,  147,   96,   61, 1013,\n",
       "          15,    3,  159,   34,   34,   20,    3,   59,   26,   29,    5,\n",
       "         139,  273,    1,  153,  681, 1137, 1159,   22,   10,   17,    5,\n",
       "         114,  126,   50,   20,    8,   63,   80,  120,  394,    1,  112,\n",
       "         298,  134,   10,  113,   19,  161,  156,  335,    9,    4,   20,\n",
       "          29,  334,   25,  489,   68,    6,    3,  178,   23,   31,   84,\n",
       "         108,   98,  250,   70,    1,   61,    8,   16,   59,  298,   23,\n",
       "           1,   10,    7,  259,    2,  270,  101,    7,  151, 5466,   56,\n",
       "           4,    6,  158,   97,  109,  248,  148,   52,  436,   24,  173,\n",
       "           7,  399,  438,  335,  192, 1228,   46, 1640,   27,   77,   19,\n",
       "           8,    6,    2,   23,   23,    2,  184,   33,  129,   17,   53,\n",
       "           5,  250,    4,   25,  379,  157,   14,   43,  109,  584,    5,\n",
       "        2223,    1,  105,   12,    6,  130,   10,    5, 1399,    4,    5,\n",
       "          53,  388,    3, 2488, 1913, 1849,   53,  347,   13,  304,   19,\n",
       "         295,    8,   13,   69,   14,    4,    9,   26,    4,   36,   53,\n",
       "          23,    4,   17,   46,   11,    6,   33,   43,    5, 1376,  124,\n",
       "          72,  409, 1157,    1,    9,   89,   10, 3089,  103,   11,   20,\n",
       "          14]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train = utils.load_data('data/train_data_complaints_repeats_doctors.csv')\n",
    "train_topics = np.load('data/topics_train_ngramm.npy')\n",
    "train = join_topics(train, train_topics)\n",
    "train_y, pop_diagnoses, most_pop_diagnose = preproc_target_train(train, reduce_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "61976/61976 [==============================] - 13s 215us/step - loss: 5.8733 - acc: 0.0998\n",
      "Epoch 2/100\n",
      "61976/61976 [==============================] - 13s 212us/step - loss: 5.5176 - acc: 0.1808\n",
      "Epoch 3/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 5.1080 - acc: 0.2237\n",
      "Epoch 4/100\n",
      "61976/61976 [==============================] - 13s 212us/step - loss: 4.7338 - acc: 0.2444\n",
      "Epoch 5/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 4.4498 - acc: 0.2577\n",
      "Epoch 6/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 4.2481 - acc: 0.2679\n",
      "Epoch 7/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 4.1090 - acc: 0.2773\n",
      "Epoch 8/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 4.0043 - acc: 0.2860\n",
      "Epoch 9/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.9136 - acc: 0.2952\n",
      "Epoch 10/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.8513 - acc: 0.3007\n",
      "Epoch 11/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.7937 - acc: 0.3045\n",
      "Epoch 12/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.7485 - acc: 0.3090\n",
      "Epoch 13/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.7130 - acc: 0.3135\n",
      "Epoch 14/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.6764 - acc: 0.3183\n",
      "Epoch 15/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.6508 - acc: 0.3216\n",
      "Epoch 16/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.6254 - acc: 0.3243\n",
      "Epoch 17/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.6010 - acc: 0.3273\n",
      "Epoch 18/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.5754 - acc: 0.3303\n",
      "Epoch 19/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.5600 - acc: 0.3320\n",
      "Epoch 20/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.5423 - acc: 0.3338\n",
      "Epoch 21/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.5206 - acc: 0.3362\n",
      "Epoch 22/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.5016 - acc: 0.3372\n",
      "Epoch 23/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.4885 - acc: 0.3391\n",
      "Epoch 24/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4783 - acc: 0.3425\n",
      "Epoch 25/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4684 - acc: 0.3435\n",
      "Epoch 26/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4551 - acc: 0.3455\n",
      "Epoch 27/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.4406 - acc: 0.3463\n",
      "Epoch 28/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.4187 - acc: 0.3492\n",
      "Epoch 29/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.4105 - acc: 0.3511\n",
      "Epoch 30/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4028 - acc: 0.3520\n",
      "Epoch 31/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.3930 - acc: 0.3535\n",
      "Epoch 32/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.3829 - acc: 0.3549\n",
      "Epoch 33/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3741 - acc: 0.3567\n",
      "Epoch 34/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3705 - acc: 0.3569\n",
      "Epoch 35/100\n",
      "61976/61976 [==============================] - 13s 212us/step - loss: 3.3674 - acc: 0.3575\n",
      "Epoch 36/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.3516 - acc: 0.3589\n",
      "Epoch 37/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3422 - acc: 0.3609\n",
      "Epoch 38/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3366 - acc: 0.3610\n",
      "Epoch 39/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.3263 - acc: 0.3618\n",
      "Epoch 40/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.3232 - acc: 0.3636\n",
      "Epoch 41/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.3110 - acc: 0.3647\n",
      "Epoch 42/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3067 - acc: 0.3649\n",
      "Epoch 43/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2982 - acc: 0.3659\n",
      "Epoch 44/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2923 - acc: 0.3672\n",
      "Epoch 45/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2898 - acc: 0.3678\n",
      "Epoch 46/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2819 - acc: 0.3696\n",
      "Epoch 47/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2814 - acc: 0.3711\n",
      "Epoch 48/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2806 - acc: 0.3720\n",
      "Epoch 49/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.2746 - acc: 0.3714\n",
      "Epoch 50/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.2675 - acc: 0.3722\n",
      "Epoch 51/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2674 - acc: 0.3726\n",
      "Epoch 52/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2611 - acc: 0.3730\n",
      "Epoch 53/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2531 - acc: 0.3746\n",
      "Epoch 54/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2521 - acc: 0.3757\n",
      "Epoch 55/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.2457 - acc: 0.3763\n",
      "Epoch 56/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.2397 - acc: 0.3763\n",
      "Epoch 57/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2336 - acc: 0.3774\n",
      "Epoch 58/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2326 - acc: 0.3780\n",
      "Epoch 59/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2253 - acc: 0.3797\n",
      "Epoch 60/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2197 - acc: 0.3799\n",
      "Epoch 61/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2116 - acc: 0.3807\n",
      "Epoch 62/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2075 - acc: 0.3818\n",
      "Epoch 63/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2062 - acc: 0.3832\n",
      "Epoch 64/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2028 - acc: 0.3816\n",
      "Epoch 65/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1995 - acc: 0.3822\n",
      "Epoch 66/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.1949 - acc: 0.3835\n",
      "Epoch 67/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1904 - acc: 0.3836\n",
      "Epoch 68/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1891 - acc: 0.3856\n",
      "Epoch 69/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1808 - acc: 0.3866\n",
      "Epoch 70/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1779 - acc: 0.3857\n",
      "Epoch 71/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1716 - acc: 0.3875\n",
      "Epoch 72/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1683 - acc: 0.3876\n",
      "Epoch 73/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1660 - acc: 0.3878\n",
      "Epoch 74/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1626 - acc: 0.3897\n",
      "Epoch 75/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.1570 - acc: 0.3903\n",
      "Epoch 76/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.1557 - acc: 0.3917\n",
      "Epoch 77/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1534 - acc: 0.3918\n",
      "Epoch 78/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1506 - acc: 0.3916\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1456 - acc: 0.3923\n",
      "Epoch 80/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1393 - acc: 0.3923\n",
      "Epoch 81/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.1382 - acc: 0.3930\n",
      "Epoch 82/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1367 - acc: 0.3942\n",
      "Epoch 83/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1333 - acc: 0.3947\n",
      "Epoch 84/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1251 - acc: 0.3940\n",
      "Epoch 85/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1240 - acc: 0.3963\n",
      "Epoch 86/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1194 - acc: 0.3956\n",
      "Epoch 87/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1168 - acc: 0.3966\n",
      "Epoch 88/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1133 - acc: 0.3970\n",
      "Epoch 89/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1103 - acc: 0.3963\n",
      "Epoch 90/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1043 - acc: 0.3972\n",
      "Epoch 91/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1059 - acc: 0.3976\n",
      "Epoch 92/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1007 - acc: 0.3990\n",
      "Epoch 93/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0974 - acc: 0.3981\n",
      "Epoch 94/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.0929 - acc: 0.3996\n",
      "Epoch 95/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.0878 - acc: 0.4004\n",
      "Epoch 96/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.0853 - acc: 0.4003\n",
      "Epoch 97/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0840 - acc: 0.4004\n",
      "Epoch 98/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0798 - acc: 0.4023\n",
      "Epoch 99/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0758 - acc: 0.4015\n",
      "Epoch 100/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0723 - acc: 0.4017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('complaints_pipe', Pipeline(memory=None,\n",
       "     steps=[('complaint_selector', ItemSelector(key='Жалобы (unigramm)')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, e...:48_2018/{epoch:02d}_{acc:.2f}.h5',\n",
       "    tb_log_dir='simple_models/log/nn14', validation_split=0.0))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = utils.load_data('data/test_data_complaints_repeats_doctors.csv')\n",
    "test_topics = np.load('data/topics_test_ngramm.npy')\n",
    "test = join_topics(test, test_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'Id_Записи': test['Id_Записи'], 'Код_диагноза': test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M42.1     3327\n",
       "J06.9     1899\n",
       "N76.0     1822\n",
       "Z32.1     1646\n",
       "N41.1     1034\n",
       "N77.1*     978\n",
       "I11        942\n",
       "M65        789\n",
       "Z00.0      779\n",
       "J35.0      609\n",
       "J00        608\n",
       "Z01.8      603\n",
       "K29.9      577\n",
       "K30        566\n",
       "D23.9      519\n",
       "H52.1      462\n",
       "N60.1      442\n",
       "M54.2      435\n",
       "J01.0      424\n",
       "M54.8      405\n",
       "G90        384\n",
       "L30.8      382\n",
       "K29.5      334\n",
       "N76.1      296\n",
       "E03.8      268\n",
       "I83.9      266\n",
       "H61.2      249\n",
       "N30.0      240\n",
       "D25        236\n",
       "E04.2      225\n",
       "          ... \n",
       "Z00.1        4\n",
       "J37.0        4\n",
       "S83.5        3\n",
       "E03.9        3\n",
       "H68.1        3\n",
       "B35.3        3\n",
       "T15.1        3\n",
       "M17          2\n",
       "M43.9        2\n",
       "L30.9        2\n",
       "Z01          2\n",
       "H66.0        2\n",
       "G93.4        2\n",
       "N47          2\n",
       "M81.0        2\n",
       "M53.1        2\n",
       "H65.1        1\n",
       "J18.0        1\n",
       "M17.0        1\n",
       "I83          1\n",
       "N64.4        1\n",
       "Z00          1\n",
       "N10          1\n",
       "H15.1        1\n",
       "M47.8        1\n",
       "H35.4        1\n",
       "D89.8        1\n",
       "M06.0        1\n",
       "J35.3        1\n",
       "Z35.8        1\n",
       "Name: Код_диагноза, Length: 212, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['Код_диагноза'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit/bow_simple_nn20_3dense_diag_all_compl_uni-n_gram_doctor_topics.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
