{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, InputLayer\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy\n",
    "\n",
    "from os.path import join, basename, exists\n",
    "from os import makedirs, listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_target_train(data, reduce_classes=False):\n",
    "    diagnoses = data['Код_диагноза'].copy()\n",
    "    \n",
    "    if reduce_classes:\n",
    "        pop_diagnoses = set(utils.get_most_popular_diagnoses(diagnoses, percent=.80))\n",
    "        most_pop_diagnose = scipy.stats.mode(diagnoses)[0][0]\n",
    "    else:\n",
    "        pop_diagnoses = set(diagnoses)\n",
    "        most_pop_diagnose = scipy.stats.mode(diagnoses)[0][0]\n",
    "    \n",
    "    diagnoses = diagnoses.apply(\n",
    "        lambda diag: diag if diag in pop_diagnoses else most_pop_diagnose\n",
    "    )\n",
    "    \n",
    "    return diagnoses, pop_diagnoses, most_pop_diagnose\n",
    "\n",
    "def preproc_target_test(data, pop_diagnoses, most_pop_diagnose):\n",
    "    diagnoses = data['Код_диагноза'].copy()\n",
    "    \n",
    "    diagnoses = diagnoses.apply(\n",
    "        lambda diag: diag if diag in pop_diagnoses else most_pop_diagnose\n",
    "    )\n",
    "    \n",
    "    return diagnoses, pop_diagnoses, most_pop_diagnose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class DoctorsPopularityTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        doctors = x.fillna('sss')\n",
    "        doctors_voc, counts = np.unique(doctors, return_counts=True)\n",
    "        self.pop_doctor = doctors_voc[np.argsort(counts)[::-1][0]]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = x.fillna('sss')\n",
    "        x[x == 'sss'] = self.pop_doctor\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GenderTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        x[x == 1] = 0\n",
    "        x[x == 2] = 1\n",
    "        \n",
    "        return np.expand_dims(x, axis=1)\n",
    "    \n",
    "class AgeTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return np.expand_dims(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, batch_size, epochs, tb_log_dir, model_path, validation_split=0.3):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.tb_log_dir = tb_log_dir\n",
    "        self.model_path = model_path\n",
    "        self.validation_split = validation_split\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        self.classes = np.unique(y)\n",
    "        self.classes_voc = dict(zip(self.classes, range(self.classes.shape[0])))\n",
    "        self.voc_classes = dict(zip(range(self.classes.shape[0]), self.classes))\n",
    "        y_proc = np.zeros((y.shape[0], self.classes.shape[0]), dtype=np.float32)\n",
    "        for i, yc in enumerate(y):\n",
    "            y_proc[i, self.classes_voc[yc]] = 1.\n",
    "        \n",
    "        self.model = Sequential([\n",
    "            InputLayer(input_shape=(x.shape[1],)),\n",
    "            Dense(4096, activation='sigmoid'),\n",
    "            Dense(self.classes.shape[0], activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        optim = RMSprop(lr=1e-4, decay=1e-6)\n",
    "        self.model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model.fit(x, y_proc,\n",
    "                  batch_size=self.batch_size, epochs=self.epochs,\n",
    "                  callbacks=[\n",
    "                      TensorBoard(log_dir=self.tb_log_dir, batch_size=self.batch_size),\n",
    "                      ModelCheckpoint(filepath=self.model_path, monitor='acc', period=5)\n",
    "                  ],\n",
    "                  validation_split=self.validation_split)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, x):\n",
    "        pred = self.model.predict(x)\n",
    "        max_idxs = np.argmax(pred, axis=1)\n",
    "        \n",
    "        return np.array(list(map(lambda max_idx: self.voc_classes[max_idx], max_idxs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = 'simple_models'\n",
    "model_name = 'nn' + str(utils.get_next_model_id(experiment_dir))\n",
    "tb_log_dir = join(experiment_dir, 'log', model_name)\n",
    "models_dir = join(experiment_dir, 'models')\n",
    "model_path = utils.get_model_fname_pattern(models_dir, model_name)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('complaints_pipe', Pipeline([\n",
    "                ('complaint_selector', ItemSelector(key='Жалобы (unigramm)')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,1), min_df=10, stop_words=stopwords.words('russian')))\n",
    "            ])),\n",
    "            ('complaints_n_pipe', Pipeline([\n",
    "                ('complaint_n_selector', ItemSelector(key='Жалобы (ngramm)')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,1), min_df=1, stop_words=stopwords.words('russian')))\n",
    "            ])),\n",
    "#             ('doctor_pipe', Pipeline([\n",
    "#                 ('doctor_selector', ItemSelector(key='Врач')),\n",
    "#                 ('doc_pop', DoctorsPopularityTransformator()),\n",
    "#                 ('count_vect', CountVectorizer())\n",
    "#             ])),\n",
    "            ('gender_pipe', Pipeline([\n",
    "                ('gender_selector', ItemSelector(key='Пол')),\n",
    "                ('gender_transform', GenderTransformator())\n",
    "            ])),\n",
    "            ('age_pipe', Pipeline([\n",
    "                ('age_selector', ItemSelector(key='Возраст')),\n",
    "                ('age_transformator', AgeTransformator())\n",
    "            ]))\n",
    "        ]\n",
    "    )),\n",
    "    ('clf', NNModel(batch_size=128, epochs=100, tb_log_dir=tb_log_dir,\n",
    "                    model_path=model_path, validation_split=0.3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = utils.load_data('data/train_data_complaints_repeats_doctors.csv')\n",
    "# train, valid = train_test_split(train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train_y, pop_diagnoses, most_pop_diagnose = preproc_target_train(train, reduce_classes=False)\n",
    "# valid_y, _, _ = preproc_target_test(valid, pop_diagnoses, most_pop_diagnose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2302,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43383 samples, validate on 18593 samples\n",
      "Epoch 1/100\n",
      "43383/43383 [==============================] - 10s 227us/step - loss: 5.7470 - acc: 0.0705 - val_loss: 6.6203 - val_acc: 0.0830\n",
      "Epoch 2/100\n",
      "43383/43383 [==============================] - 9s 219us/step - loss: 5.5697 - acc: 0.1133 - val_loss: 6.5280 - val_acc: 0.1198\n",
      "Epoch 3/100\n",
      "43383/43383 [==============================] - 10s 236us/step - loss: 5.4074 - acc: 0.1464 - val_loss: 6.3704 - val_acc: 0.1404\n",
      "Epoch 4/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 5.2254 - acc: 0.1774 - val_loss: 6.2335 - val_acc: 0.1585\n",
      "Epoch 5/100\n",
      "43383/43383 [==============================] - 10s 226us/step - loss: 5.0578 - acc: 0.2015 - val_loss: 6.0922 - val_acc: 0.1748\n",
      "Epoch 6/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 4.8932 - acc: 0.2214 - val_loss: 5.9440 - val_acc: 0.1991\n",
      "Epoch 7/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 4.7370 - acc: 0.2322 - val_loss: 5.8037 - val_acc: 0.2053\n",
      "Epoch 8/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 4.5947 - acc: 0.2431 - val_loss: 5.6719 - val_acc: 0.2174\n",
      "Epoch 9/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 4.4715 - acc: 0.2525 - val_loss: 5.5684 - val_acc: 0.2170\n",
      "Epoch 10/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 4.3626 - acc: 0.2589 - val_loss: 5.4776 - val_acc: 0.2253\n",
      "Epoch 11/100\n",
      "43383/43383 [==============================] - 9s 218us/step - loss: 4.2679 - acc: 0.2652 - val_loss: 5.4060 - val_acc: 0.2274\n",
      "Epoch 12/100\n",
      "43383/43383 [==============================] - 10s 219us/step - loss: 4.1805 - acc: 0.2723 - val_loss: 5.3399 - val_acc: 0.2354\n",
      "Epoch 13/100\n",
      "43383/43383 [==============================] - 9s 217us/step - loss: 4.1118 - acc: 0.2768 - val_loss: 5.2650 - val_acc: 0.2370\n",
      "Epoch 14/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 4.0392 - acc: 0.2810 - val_loss: 5.2165 - val_acc: 0.2441\n",
      "Epoch 15/100\n",
      "43383/43383 [==============================] - 9s 218us/step - loss: 3.9738 - acc: 0.2861 - val_loss: 5.1544 - val_acc: 0.2468\n",
      "Epoch 16/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.9207 - acc: 0.2895 - val_loss: 5.1165 - val_acc: 0.2487\n",
      "Epoch 17/100\n",
      "43383/43383 [==============================] - 9s 219us/step - loss: 3.8656 - acc: 0.2933 - val_loss: 5.0709 - val_acc: 0.2501\n",
      "Epoch 18/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.8192 - acc: 0.2986 - val_loss: 5.0325 - val_acc: 0.2534\n",
      "Epoch 19/100\n",
      "43383/43383 [==============================] - 10s 225us/step - loss: 3.7737 - acc: 0.3010 - val_loss: 5.0005 - val_acc: 0.2591\n",
      "Epoch 20/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 3.7382 - acc: 0.3056 - val_loss: 4.9711 - val_acc: 0.2535\n",
      "Epoch 21/100\n",
      "43383/43383 [==============================] - 10s 225us/step - loss: 3.7032 - acc: 0.3063 - val_loss: 4.9394 - val_acc: 0.2611\n",
      "Epoch 22/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.6696 - acc: 0.3106 - val_loss: 4.9164 - val_acc: 0.2655\n",
      "Epoch 23/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.6361 - acc: 0.3129 - val_loss: 4.8848 - val_acc: 0.2617\n",
      "Epoch 24/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.6048 - acc: 0.3165 - val_loss: 4.8693 - val_acc: 0.2674\n",
      "Epoch 25/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 3.5829 - acc: 0.3193 - val_loss: 4.8392 - val_acc: 0.2721\n",
      "Epoch 26/100\n",
      "43383/43383 [==============================] - 9s 218us/step - loss: 3.5566 - acc: 0.3197 - val_loss: 4.8287 - val_acc: 0.2690\n",
      "Epoch 27/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.5323 - acc: 0.3222 - val_loss: 4.8115 - val_acc: 0.2692\n",
      "Epoch 28/100\n",
      "43383/43383 [==============================] - 10s 227us/step - loss: 3.5090 - acc: 0.3247 - val_loss: 4.7775 - val_acc: 0.2726\n",
      "Epoch 29/100\n",
      "43383/43383 [==============================] - 10s 224us/step - loss: 3.4830 - acc: 0.3263 - val_loss: 4.7679 - val_acc: 0.2741\n",
      "Epoch 30/100\n",
      "43383/43383 [==============================] - 10s 224us/step - loss: 3.4645 - acc: 0.3292 - val_loss: 4.7622 - val_acc: 0.2738\n",
      "Epoch 31/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.4500 - acc: 0.3298 - val_loss: 4.7566 - val_acc: 0.2749\n",
      "Epoch 32/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 3.4327 - acc: 0.3319 - val_loss: 4.7433 - val_acc: 0.2745\n",
      "Epoch 33/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.4159 - acc: 0.3335 - val_loss: 4.7370 - val_acc: 0.2726\n",
      "Epoch 34/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.3960 - acc: 0.3352 - val_loss: 4.7251 - val_acc: 0.2793\n",
      "Epoch 35/100\n",
      "43383/43383 [==============================] - 10s 230us/step - loss: 3.3874 - acc: 0.3366 - val_loss: 4.6927 - val_acc: 0.2791\n",
      "Epoch 36/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 3.3651 - acc: 0.3367 - val_loss: 4.6951 - val_acc: 0.2793\n",
      "Epoch 37/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.3484 - acc: 0.3415 - val_loss: 4.6881 - val_acc: 0.2808\n",
      "Epoch 38/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 3.3374 - acc: 0.3409 - val_loss: 4.6731 - val_acc: 0.2812\n",
      "Epoch 39/100\n",
      "43383/43383 [==============================] - 10s 238us/step - loss: 3.3252 - acc: 0.3433 - val_loss: 4.6633 - val_acc: 0.2815\n",
      "Epoch 40/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 3.3106 - acc: 0.3437 - val_loss: 4.6656 - val_acc: 0.2804\n",
      "Epoch 41/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 3.3018 - acc: 0.3464 - val_loss: 4.6677 - val_acc: 0.2800\n",
      "Epoch 42/100\n",
      "43383/43383 [==============================] - 9s 219us/step - loss: 3.2946 - acc: 0.3480 - val_loss: 4.6450 - val_acc: 0.2818\n",
      "Epoch 43/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.2786 - acc: 0.3491 - val_loss: 4.6368 - val_acc: 0.2801\n",
      "Epoch 44/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.2634 - acc: 0.3496 - val_loss: 4.6370 - val_acc: 0.2857\n",
      "Epoch 45/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.2522 - acc: 0.3535 - val_loss: 4.6331 - val_acc: 0.2833\n",
      "Epoch 46/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.2419 - acc: 0.3530 - val_loss: 4.6226 - val_acc: 0.2857\n",
      "Epoch 47/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.2358 - acc: 0.3553 - val_loss: 4.6192 - val_acc: 0.2841\n",
      "Epoch 48/100\n",
      "43383/43383 [==============================] - 10s 219us/step - loss: 3.2202 - acc: 0.3558 - val_loss: 4.6124 - val_acc: 0.2838\n",
      "Epoch 49/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 3.2054 - acc: 0.3575 - val_loss: 4.6077 - val_acc: 0.2848\n",
      "Epoch 50/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 3.1959 - acc: 0.3581 - val_loss: 4.6070 - val_acc: 0.2856\n",
      "Epoch 51/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.1936 - acc: 0.3594 - val_loss: 4.6021 - val_acc: 0.2863\n",
      "Epoch 52/100\n",
      "43383/43383 [==============================] - 9s 218us/step - loss: 3.1774 - acc: 0.3608 - val_loss: 4.5906 - val_acc: 0.2862\n",
      "Epoch 53/100\n",
      "43383/43383 [==============================] - 10s 234us/step - loss: 3.1697 - acc: 0.3618 - val_loss: 4.5869 - val_acc: 0.2876\n",
      "Epoch 54/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 3.1556 - acc: 0.3638 - val_loss: 4.5829 - val_acc: 0.2885\n",
      "Epoch 55/100\n",
      "43383/43383 [==============================] - 9s 219us/step - loss: 3.1465 - acc: 0.3650 - val_loss: 4.5676 - val_acc: 0.2856\n",
      "Epoch 56/100\n",
      "43383/43383 [==============================] - 10s 234us/step - loss: 3.1350 - acc: 0.3656 - val_loss: 4.5638 - val_acc: 0.2872\n",
      "Epoch 57/100\n",
      "43383/43383 [==============================] - 11s 247us/step - loss: 3.1294 - acc: 0.3665 - val_loss: 4.5661 - val_acc: 0.2899\n",
      "Epoch 58/100\n",
      "43383/43383 [==============================] - 10s 236us/step - loss: 3.1182 - acc: 0.3684 - val_loss: 4.5698 - val_acc: 0.2871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.1108 - acc: 0.3677 - val_loss: 4.5739 - val_acc: 0.2892\n",
      "Epoch 60/100\n",
      "43383/43383 [==============================] - 10s 237us/step - loss: 3.1060 - acc: 0.3697 - val_loss: 4.5757 - val_acc: 0.2860\n",
      "Epoch 61/100\n",
      "43383/43383 [==============================] - 10s 225us/step - loss: 3.1015 - acc: 0.3713 - val_loss: 4.5740 - val_acc: 0.2863\n",
      "Epoch 62/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.1004 - acc: 0.3709 - val_loss: 4.5529 - val_acc: 0.2889\n",
      "Epoch 63/100\n",
      "43383/43383 [==============================] - 10s 231us/step - loss: 3.0840 - acc: 0.3729 - val_loss: 4.5551 - val_acc: 0.2921\n",
      "Epoch 64/100\n",
      "43383/43383 [==============================] - 10s 224us/step - loss: 3.0821 - acc: 0.3721 - val_loss: 4.5493 - val_acc: 0.2885\n",
      "Epoch 65/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.0737 - acc: 0.3741 - val_loss: 4.5515 - val_acc: 0.2906\n",
      "Epoch 66/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.0606 - acc: 0.3746 - val_loss: 4.5392 - val_acc: 0.2895\n",
      "Epoch 67/100\n",
      "43383/43383 [==============================] - 10s 222us/step - loss: 3.0508 - acc: 0.3770 - val_loss: 4.5402 - val_acc: 0.2882\n",
      "Epoch 68/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 3.0480 - acc: 0.3756 - val_loss: 4.5396 - val_acc: 0.2888\n",
      "Epoch 69/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 3.0449 - acc: 0.3774 - val_loss: 4.5379 - val_acc: 0.2890\n",
      "Epoch 70/100\n",
      "43383/43383 [==============================] - 10s 221us/step - loss: 3.0323 - acc: 0.3790 - val_loss: 4.5430 - val_acc: 0.2910\n",
      "Epoch 71/100\n",
      "43383/43383 [==============================] - 10s 223us/step - loss: 3.0315 - acc: 0.3812 - val_loss: 4.5400 - val_acc: 0.2892\n",
      "Epoch 72/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.0242 - acc: 0.3803 - val_loss: 4.5434 - val_acc: 0.2900\n",
      "Epoch 73/100\n",
      "43383/43383 [==============================] - 10s 228us/step - loss: 3.0218 - acc: 0.3810 - val_loss: 4.5306 - val_acc: 0.2898\n",
      "Epoch 74/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.0086 - acc: 0.3821 - val_loss: 4.5234 - val_acc: 0.2896\n",
      "Epoch 75/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 3.0023 - acc: 0.3841 - val_loss: 4.5328 - val_acc: 0.2888\n",
      "Epoch 76/100\n",
      "43383/43383 [==============================] - 10s 220us/step - loss: 2.9996 - acc: 0.3839 - val_loss: 4.5159 - val_acc: 0.2898\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipe.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A46', 'A49.3', 'A63.0', 'A63.8', 'A66.1', 'B00', 'B00.1', 'B00.8',\n",
       "        'B00.9', 'B01.9', 'B02', 'B02.9', 'B07', 'B08.1', 'B35.1', 'B35.3',\n",
       "        'B35.4', 'B35.6', 'B36.0', 'B37', 'B37.2', 'B37.3+', 'B37.4+',\n",
       "        'B97.1', 'B97.7', 'C20', 'C44.5', 'C61', 'D17.0', 'D17.1', 'D17.2',\n",
       "        'D18.0', 'D21', 'D22.5', 'D23', 'D23.3', 'D23.4', 'D23.5', 'D23.6',\n",
       "        'D23.7', 'D23.9', 'D24', 'D25', 'D32.1', 'D35.2', 'D50', 'D50.0',\n",
       "        'D50.9', 'D64.9', 'D72.1', 'D80.2', 'D89.8', 'E01.0', 'E01.1',\n",
       "        'E01.8', 'E03.8', 'E03.9', 'E04.1', 'E04.2', 'E05', 'E05.0',\n",
       "        'E05.8', 'E05.9', 'E06.1', 'E06.3', 'E10', 'E11', 'E13', 'E14',\n",
       "        'E22.1', 'E28', 'E28.1', 'E28.8', 'E28.9', 'E29.1', 'E34', 'E55',\n",
       "        'E66', 'E66.0', 'E78.0', 'E89.0', 'F34.1', 'F40.0', 'F41.0',\n",
       "        'F41.1', 'F41.2', 'F41.9', 'F43.2', 'F45', 'F45.3', 'F45.9',\n",
       "        'F52.2', 'F95.0', 'G24', 'G24.9', 'G43.0', 'G43.1', 'G44.1',\n",
       "        'G44.2', 'G45.0', 'G50', 'G50.0', 'G50.8', 'G51', 'G51.0',\n",
       "        'G53.0*', 'G55*', 'G55.1*', 'G55.3*', 'G56.0', 'G56.1', 'G56.2',\n",
       "        'G56.3', 'G57', 'G58.0', 'G58.8', 'G63.2*', 'G90', 'G90.8',\n",
       "        'G90.9', 'G93.4', 'G93.8', 'H00', 'H00.0', 'H00.1', 'H01.0',\n",
       "        'H01.1', 'H01.8', 'H02.4', 'H04.1', 'H04.8', 'H10.0', 'H10.1',\n",
       "        'H10.2', 'H10.3', 'H10.5', 'H10.8', 'H11.3', 'H15.1', 'H16.1',\n",
       "        'H16.2', 'H20.0', 'H25.0', 'H35.0', 'H35.3', 'H35.4', 'H40.1',\n",
       "        'H43.1', 'H43.3', 'H43.8', 'H52.0', 'H52.1', 'H52.2', 'H52.3',\n",
       "        'H52.4', 'H52.5', 'H53.1', 'H60', 'H61.2', 'H65', 'H65.0', 'H65.1',\n",
       "        'H66.0', 'H66.1', 'H68', 'H68.0', 'H68.1', 'H81.1', 'H81.2',\n",
       "        'H81.8', 'H90.3', 'H93.1', 'I10', 'I11', 'I11.9', 'I15', 'I20',\n",
       "        'I20.1', 'I20.8', 'I20.9', 'I25', 'I25.0', 'I25.1', 'I25.2',\n",
       "        'I34.1', 'I42.9', 'I45.8', 'I47', 'I48', 'I49', 'I49.8', 'I49.9',\n",
       "        'I67.2', 'I67.4', 'I67.8', 'I67.9', 'I69.3', 'I69.4', 'I70',\n",
       "        'I70.2', 'I80.0', 'I80.8', 'I82.8', 'I83', 'I83.9', 'I84.1',\n",
       "        'I84.2', 'I84.3', 'I84.4', 'I84.5', 'I86.1', 'I87.0', 'I87.2',\n",
       "        'I88', 'J00', 'J01', 'J01.0', 'J01.1', 'J01.3', 'J01.8', 'J02',\n",
       "        'J02.9', 'J03', 'J03.9', 'J04.0', 'J04.1', 'J04.2', 'J06', 'J06.0',\n",
       "        'J06.9', 'J18', 'J18.0', 'J18.9', 'J20', 'J20.9', 'J30', 'J30.0',\n",
       "        'J30.1', 'J30.3', 'J30.4', 'J31.0', 'J31.1', 'J31.2', 'J32',\n",
       "        'J32.0', 'J34.0', 'J34.1', 'J34.2', 'J34.8', 'J35.0', 'J35.2',\n",
       "        'J35.3', 'J35.8', 'J37.0', 'J41.0', 'J42', 'J44.9', 'J45.0',\n",
       "        'J45.8', 'J45.9', 'K02.1', 'K04.0', 'K04.5', 'K05.1', 'K05.2',\n",
       "        'K05.3', 'K07.6', 'K21', 'K21.0', 'K21.9', 'K25', 'K25.7', 'K25.9',\n",
       "        'K26', 'K26.7', 'K29', 'K29.1', 'K29.3', 'K29.5', 'K29.6', 'K29.9',\n",
       "        'K30', 'K35', 'K40', 'K40.9', 'K42', 'K42.9', 'K43', 'K43.9',\n",
       "        'K46', 'K51.2', 'K51.9', 'K52', 'K52.9', 'K57', 'K57.3', 'K58',\n",
       "        'K58.0', 'K58.9', 'K59.9', 'K60.0', 'K60.1', 'K60.2', 'K63.8',\n",
       "        'K63.9', 'K73.9', 'K76.0', 'K80', 'K80.0', 'K80.1', 'K81', 'K81.9',\n",
       "        'K82', 'K82.8', 'K83', 'K83.8', 'K83.9', 'K86.1', 'K87*', 'K91.5',\n",
       "        'L01', 'L01.0', 'L01.1', 'L02', 'L02.0', 'L02.1', 'L02.2', 'L02.3',\n",
       "        'L02.4', 'L02.8', 'L04.0', 'L05', 'L05.0', 'L05.9', 'L08.0',\n",
       "        'L08.8', 'L08.9', 'L20', 'L20.8', 'L21', 'L23', 'L23.2', 'L23.8',\n",
       "        'L23.9', 'L24.3', 'L28.0', 'L30.0', 'L30.1', 'L30.2', 'L30.3',\n",
       "        'L30.8', 'L30.9', 'L40', 'L40.0', 'L50.9', 'L52', 'L60.0', 'L60.3',\n",
       "        'L63', 'L64', 'L65.0', 'L70.0', 'L71', 'L71.0', 'L72', 'L72.0',\n",
       "        'L72.1', 'L73.2', 'L82', 'L84', 'L85.3', 'L87.0', 'L91.0', 'L95.9',\n",
       "        'L98.6', 'M02.8', 'M05', 'M05.8', 'M06.0', 'M07.0*', 'M12.5',\n",
       "        'M13', 'M15', 'M15-M19', 'M15.8', 'M15.9', 'M16', 'M16.0', 'M16.9',\n",
       "        'M17', 'M17.0', 'M17.1', 'M17.3', 'M17.4', 'M17.5', 'M17.9', 'M19',\n",
       "        'M19.0', 'M20-M25', 'M20.1', 'M21.4', 'M22.4', 'M23.2', 'M24.5',\n",
       "        'M32', 'M41', 'M41.1', 'M42', 'M42.0', 'M42.1', 'M43.9', 'M45',\n",
       "        'M47', 'M47.0+', 'M47.8', 'M48.0', 'M50', 'M50.1', 'M50.2',\n",
       "        'M50.3', 'M51', 'M51.0+', 'M51.1', 'M51.2', 'M51.3', 'M51.8',\n",
       "        'M53.0', 'M53.1', 'M53.8', 'M53.9', 'M54', 'M54.1', 'M54.2',\n",
       "        'M54.3', 'M54.4', 'M54.5', 'M54.6', 'M54.8', 'M54.9', 'M60', 'M65',\n",
       "        'M65-M68', 'M65.8', 'M67.4', 'M70.1', 'M70.2', 'M70.6', 'M70.8',\n",
       "        'M71.3', 'M71.8', 'M72.5', 'M75.0', 'M75.3', 'M76.0', 'M76.5',\n",
       "        'M76.6', 'M77.0', 'M77.1', 'M77.3', 'M77.4', 'M77.9', 'M79.1',\n",
       "        'M79.2', 'M79.6', 'M81', 'M81.0', 'N10', 'N11', 'N11.0', 'N11.9',\n",
       "        'N20', 'N20.0', 'N20.1', 'N20.2', 'N28.1', 'N30', 'N30.0', 'N30.1',\n",
       "        'N30.2', 'N30.8', 'N34', 'N34.1', 'N34.2', 'N34.3', 'N37.0*',\n",
       "        'N39.0', 'N40', 'N41', 'N41.0', 'N41.1', 'N42', 'N42.8', 'N43.0',\n",
       "        'N43.4', 'N45', 'N45.9', 'N46', 'N47', 'N48.1', 'N48.6', 'N48.8',\n",
       "        'N60.1', 'N64', 'N64.4', 'N70', 'N70.1', 'N71', 'N72', 'N75.0',\n",
       "        'N76.0', 'N76.1', 'N76.2', 'N76.3', 'N77.1*', 'N80', 'N80.0',\n",
       "        'N80.1', 'N83.0', 'N83.2', 'N84.0', 'N84.1', 'N85.0', 'N86',\n",
       "        'N88.0', 'N92.1', 'N92.6', 'N94.0', 'N94.6', 'N94.9', 'N95.1',\n",
       "        'N95.2', 'N97.9', 'O04', 'O20.0', 'R04.0', 'R05', 'R42', 'R45.8',\n",
       "        'R51', 'R52.0', 'R53', 'R73', 'R73.0', 'S00', 'S02.2', 'S06.0',\n",
       "        'S20.2', 'S22.0', 'S22.3', 'S30.0', 'S32.0', 'S32.1', 'S40.0',\n",
       "        'S43.4', 'S50.0', 'S52.5', 'S53', 'S61', 'S61.0', 'S62.3', 'S70.1',\n",
       "        'S80', 'S80.0', 'S80.1', 'S81', 'S83.5', 'S90.0', 'S90.1', 'S90.3',\n",
       "        'S91', 'S91.1', 'S92.0', 'S92.3', 'S92.5', 'S93.2', 'S93.3',\n",
       "        'S93.4', 'T15.1', 'T90.8', 'T92', 'T93', 'T93.2', 'W57', 'Z00',\n",
       "        'Z00.0', 'Z00.1', 'Z00.8', 'Z01', 'Z01.0', 'Z01.4', 'Z01.7',\n",
       "        'Z01.8', 'Z03.3', 'Z04.8', 'Z10', 'Z10.0', 'Z30', 'Z30.5', 'Z32.1',\n",
       "        'Z34', 'Z34.0', 'Z34.8', 'Z35', 'Z35.8', 'Z39.2'], dtype='<U7'),\n",
       " array([  30,    1,   13,    2,    1,  111,   18,   29,    3,    1,  108,\n",
       "           8,  331,   23,   55,   47,    2,   44,  147,   14,    6,  203,\n",
       "           5,    7,   60,    2,    1,    2,    8,   25,   35,   18,    1,\n",
       "           3,  139,    2,    1,   88,    7,    5,  752,   35,  326,    4,\n",
       "           8,   79,    5,   38,    1,    3,    2,   22,    2,   25,   20,\n",
       "         279,   39,   89,  432,    4,   47,    2,    4,    1,   84,    5,\n",
       "         209,    5,    8,    8,  124,    5,   27,   12,    3,    1,   29,\n",
       "           1,   88,    2,   24,    2,    3,   30,   16,   20,    1,    1,\n",
       "           8,   88,    3,    4,   15,   26,   12,   18,    4,    7,  198,\n",
       "          16,    6,   63,    5,    7,   16,    9,   42,   20,   41,    8,\n",
       "           1,   34,    4,   18,   14,    1,    4,  401,   88,   65,   57,\n",
       "           2,   33,   42,   21,   15,    1,    1,   10,   41,    2,   30,\n",
       "          60,  154,   27,   20,    9,   15,   39,    9,    8,   14,    1,\n",
       "         356,   10,   37,    1,    6,    1,    2,   82,  670,  102,   18,\n",
       "         121,   38,    1,  245,  491,   39,   28,   10,   32,   16,    8,\n",
       "          11,   48,   11,    9,    4,   19,   34,  167, 1537,  171,    2,\n",
       "          55,    7,  102,  118,    3,   67,   12,   16,  149,    6,    6,\n",
       "          14,   44,    9,  121,    2,  300,   31,  194,    1,    5,    2,\n",
       "           2,   26,    5,    4,    1,    7,  676,    6,    6,   76,   48,\n",
       "         117,   15,   22,   19,    4, 1446,    5,  608,   10,    1,   32,\n",
       "          86,   21,   56,   48,   36,  464,    5,   25,   47, 2918,  207,\n",
       "          19,  105,   19,  111,   13,   75,   74,  136,    4,  187,  123,\n",
       "         155,    1,    5,    3,    3,  145,    5,  914,   23,    7,  220,\n",
       "           6,   34,    1,    3,   34,   20,   35,   55,   24,   11,   23,\n",
       "           4,   12,    1,  113,  414,   15,    2,   11,    3,    2,    6,\n",
       "           2,    2,  157,  597,   19, 1012, 1078,    3,   19,   15,   10,\n",
       "           1,   14,    4,   26,   12,    1,    1,   84,    2,    2,  197,\n",
       "          40,   10,    3,    3,    1,    8,    9,    1,    6,    1,    2,\n",
       "           5,   43,   13,    4,    1,   65,    2,    1,  107,  235,  235,\n",
       "           2,    2,    3,    1,    1,   76,    4,  313,   15,  179,    3,\n",
       "          22,    1,   10,    5,  159,   16,    1,  127,  135,  304,   50,\n",
       "           2,    2,   12,    1,   17,   71,    8,  240,   28,  333,   63,\n",
       "           1,   29,    9,    8,  166,   36,   32,    3,  118,  151,   50,\n",
       "         293,   70,    4,    4,    8,    1,   40,    4,    3,   22,    7,\n",
       "           3,    1,    3,    7,   21,    2,   81,    1,    5,  136,  274,\n",
       "           3,    9,    5,    3,   68,   10,   15,    2,   14,    2,    1,\n",
       "          13,    3,   19,    1,  237,    3,  264,    2,    4,  165,   26,\n",
       "         160,    1, 5370,   10,   12,  136,    1,   23,    3,   16,   21,\n",
       "           6,    3,  578,    5,  143,   13,  144,  110,  143,   80,  621,\n",
       "          21,  159,   39,  347,   17,  386,  349,  221,  908,  173,    1,\n",
       "        1124,    8,    5,   27,    1,   50,    3,   13,    3,   74,   12,\n",
       "           1,   18,   61,   17,   10,    1,   46,   59,    1,   11,  296,\n",
       "          47,  139,    1,   48,   27,   32,    4,    7,   12,  238,    9,\n",
       "          19,    4,   32,  370,   15,  399,    1,   30,   60,   73,    1,\n",
       "           3,    1,  357,    4,   16, 1697,    3,  125,   11,    3,    2,\n",
       "           5,   14,   16,  116,    2,    8,  881,    9,   24,   52,    3,\n",
       "           3,  299,   15, 3774,  261,   11,    5, 1898,    1,  146,    1,\n",
       "         427,   43,  205,   12,   67,  181,   13,    1,   10,    6,    3,\n",
       "          32,   29,  101,    3,   39,    1,   14,    7,    5,   10,    7,\n",
       "          23,    3,    8,    2,    1,    2,   35,   15,    1,   17,    3,\n",
       "           1,    6,    4,    7,    5,   45,    1,    9,   32,   17,    2,\n",
       "          11,   63,    6,    9,   20,    3,    1,    1,    2,    1,    2,\n",
       "          20,   15,    8,    2,   36,   26,    5,    2,    8,    1,    1,\n",
       "          77, 1001,   18,  119,   23,  153,  372,   16, 1012,   21,   41,\n",
       "           1,  149,    1,   11, 2689,    9,  129,   68,   20,   20,    6]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train = utils.load_data('data/train_data_complaints_repeats_doctors.csv')\n",
    "train_y, pop_diagnoses, most_pop_diagnose = preproc_target_train(train, reduce_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 5.8942 - acc: 0.0683\n",
      "Epoch 2/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 5.6908 - acc: 0.1080\n",
      "Epoch 3/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 5.5446 - acc: 0.1354\n",
      "Epoch 4/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 5.3526 - acc: 0.1662\n",
      "Epoch 5/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 5.1235 - acc: 0.1936\n",
      "Epoch 6/100\n",
      "61976/61976 [==============================] - 6s 91us/step - loss: 4.8901 - acc: 0.2130\n",
      "Epoch 7/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 4.6735 - acc: 0.2288\n",
      "Epoch 8/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 4.4935 - acc: 0.2394\n",
      "Epoch 9/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 4.3432 - acc: 0.2480\n",
      "Epoch 10/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 4.2246 - acc: 0.2549\n",
      "Epoch 11/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 4.1261 - acc: 0.2628\n",
      "Epoch 12/100\n",
      "61976/61976 [==============================] - 6s 92us/step - loss: 4.0460 - acc: 0.2682\n",
      "Epoch 13/100\n",
      "61976/61976 [==============================] - 6s 91us/step - loss: 3.9829 - acc: 0.2729\n",
      "Epoch 14/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.9260 - acc: 0.2784\n",
      "Epoch 15/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.8750 - acc: 0.2829\n",
      "Epoch 16/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.8325 - acc: 0.2886\n",
      "Epoch 17/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.7941 - acc: 0.2919\n",
      "Epoch 18/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.7600 - acc: 0.2958\n",
      "Epoch 19/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.7294 - acc: 0.2999\n",
      "Epoch 20/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.7016 - acc: 0.3029\n",
      "Epoch 21/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.6740 - acc: 0.3066\n",
      "Epoch 22/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.6530 - acc: 0.3077\n",
      "Epoch 23/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.6298 - acc: 0.3102\n",
      "Epoch 24/100\n",
      "61976/61976 [==============================] - 6s 92us/step - loss: 3.6097 - acc: 0.3133\n",
      "Epoch 25/100\n",
      "61976/61976 [==============================] - 6s 91us/step - loss: 3.5913 - acc: 0.3145\n",
      "Epoch 26/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.5711 - acc: 0.3163\n",
      "Epoch 27/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.5560 - acc: 0.3188\n",
      "Epoch 28/100\n",
      "61976/61976 [==============================] - 6s 92us/step - loss: 3.5400 - acc: 0.3198\n",
      "Epoch 29/100\n",
      "61976/61976 [==============================] - 6s 90us/step - loss: 3.5240 - acc: 0.3216\n",
      "Epoch 30/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.5106 - acc: 0.3235\n",
      "Epoch 31/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.4973 - acc: 0.3248\n",
      "Epoch 32/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.4838 - acc: 0.3256\n",
      "Epoch 33/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.4702 - acc: 0.3274\n",
      "Epoch 34/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.4580 - acc: 0.3290\n",
      "Epoch 35/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.4466 - acc: 0.3296\n",
      "Epoch 36/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.4369 - acc: 0.3307\n",
      "Epoch 37/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.4268 - acc: 0.3311\n",
      "Epoch 38/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.4178 - acc: 0.3335\n",
      "Epoch 39/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.4109 - acc: 0.3334\n",
      "Epoch 40/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.4036 - acc: 0.3339\n",
      "Epoch 41/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.3944 - acc: 0.3356\n",
      "Epoch 42/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.3864 - acc: 0.3362\n",
      "Epoch 43/100\n",
      "61976/61976 [==============================] - 6s 92us/step - loss: 3.3779 - acc: 0.3371\n",
      "Epoch 44/100\n",
      "61976/61976 [==============================] - 6s 92us/step - loss: 3.3701 - acc: 0.3376\n",
      "Epoch 45/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3641 - acc: 0.3403\n",
      "Epoch 46/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3567 - acc: 0.3402\n",
      "Epoch 47/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3511 - acc: 0.3421\n",
      "Epoch 48/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3451 - acc: 0.3420\n",
      "Epoch 49/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.3399 - acc: 0.3433\n",
      "Epoch 50/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3335 - acc: 0.3443\n",
      "Epoch 51/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3275 - acc: 0.3456\n",
      "Epoch 52/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3228 - acc: 0.3446\n",
      "Epoch 53/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3187 - acc: 0.3464\n",
      "Epoch 54/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3131 - acc: 0.3467\n",
      "Epoch 55/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.3092 - acc: 0.3472\n",
      "Epoch 56/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.3043 - acc: 0.3485\n",
      "Epoch 57/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2968 - acc: 0.3486\n",
      "Epoch 58/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2927 - acc: 0.3497\n",
      "Epoch 59/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2871 - acc: 0.3501\n",
      "Epoch 60/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2805 - acc: 0.3504\n",
      "Epoch 61/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2767 - acc: 0.3510\n",
      "Epoch 62/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2708 - acc: 0.3524\n",
      "Epoch 63/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2675 - acc: 0.3525\n",
      "Epoch 64/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2631 - acc: 0.3534\n",
      "Epoch 65/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2566 - acc: 0.3536\n",
      "Epoch 66/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2519 - acc: 0.3547\n",
      "Epoch 67/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2458 - acc: 0.3545\n",
      "Epoch 68/100\n",
      "61976/61976 [==============================] - 6s 92us/step - loss: 3.2429 - acc: 0.3554\n",
      "Epoch 69/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2402 - acc: 0.3559\n",
      "Epoch 70/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2345 - acc: 0.3565\n",
      "Epoch 71/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2314 - acc: 0.3564\n",
      "Epoch 72/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2259 - acc: 0.3574\n",
      "Epoch 73/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2236 - acc: 0.3573\n",
      "Epoch 74/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2173 - acc: 0.3584\n",
      "Epoch 75/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.2121 - acc: 0.3593\n",
      "Epoch 76/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2076 - acc: 0.3594\n",
      "Epoch 77/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.2045 - acc: 0.3588\n",
      "Epoch 78/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1999 - acc: 0.3604\n",
      "Epoch 79/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1957 - acc: 0.3610\n",
      "Epoch 80/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.1914 - acc: 0.3606\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.1874 - acc: 0.3607\n",
      "Epoch 82/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1842 - acc: 0.3620\n",
      "Epoch 83/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.1804 - acc: 0.3622\n",
      "Epoch 84/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.1768 - acc: 0.3624\n",
      "Epoch 85/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.1741 - acc: 0.3627\n",
      "Epoch 86/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1702 - acc: 0.3621\n",
      "Epoch 87/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1687 - acc: 0.3638\n",
      "Epoch 88/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.1635 - acc: 0.3642\n",
      "Epoch 89/100\n",
      "61976/61976 [==============================] - 6s 93us/step - loss: 3.1593 - acc: 0.3647\n",
      "Epoch 90/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1541 - acc: 0.3657\n",
      "Epoch 91/100\n",
      "61976/61976 [==============================] - 6s 92us/step - loss: 3.1491 - acc: 0.3655\n",
      "Epoch 92/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.1432 - acc: 0.3660\n",
      "Epoch 93/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1391 - acc: 0.3661\n",
      "Epoch 94/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.1333 - acc: 0.3661\n",
      "Epoch 95/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.1290 - acc: 0.3671\n",
      "Epoch 96/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1263 - acc: 0.3675\n",
      "Epoch 97/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1233 - acc: 0.3679\n",
      "Epoch 98/100\n",
      "61976/61976 [==============================] - 6s 95us/step - loss: 3.1201 - acc: 0.3686\n",
      "Epoch 99/100\n",
      "61976/61976 [==============================] - 6s 91us/step - loss: 3.1174 - acc: 0.3694\n",
      "Epoch 100/100\n",
      "61976/61976 [==============================] - 6s 94us/step - loss: 3.1138 - acc: 0.3689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('complaints_pipe', Pipeline(memory=None,\n",
       "     steps=[('complaint_selector', ItemSelector(key='Жалобы (unigramm)')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, e... NNModel(batch_size=128, epochs=100, tb_log_dir='simple_models/log/nn2',\n",
       "    validation_split=0.0))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = utils.load_data('data/test_data_complaints_repeats_doctors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'Id_Записи': test['Id_Записи'], 'Код_диагноза': test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M42.1     2960\n",
       "Z00.0     2517\n",
       "Z32.1     1761\n",
       "N76.0     1497\n",
       "J06.9     1226\n",
       "J00       1133\n",
       "K29.9     1130\n",
       "M65       1006\n",
       "N77.1*     697\n",
       "I11        591\n",
       "I83.9      556\n",
       "K30        534\n",
       "D23.9      513\n",
       "Z01.8      506\n",
       "J35.0      473\n",
       "N60.1      461\n",
       "N41.1      387\n",
       "Z01.4      386\n",
       "D25        385\n",
       "H52.1      371\n",
       "G90        355\n",
       "N95.2      339\n",
       "H35.0      310\n",
       "M54.8      300\n",
       "K29.5      269\n",
       "M54.5      264\n",
       "M54.6      260\n",
       "M41        256\n",
       "B07        235\n",
       "N30.2      234\n",
       "          ... \n",
       "L20.8        3\n",
       "I84.4        3\n",
       "I20.9        3\n",
       "N47          3\n",
       "H00.0        3\n",
       "N80.0        3\n",
       "N34.1        3\n",
       "B02          2\n",
       "D18.0        2\n",
       "A63.0        2\n",
       "L28.0        2\n",
       "L71          2\n",
       "L23          1\n",
       "M42.0        1\n",
       "J01.1        1\n",
       "S90.1        1\n",
       "L30.9        1\n",
       "M12.5        1\n",
       "J45.8        1\n",
       "G93.4        1\n",
       "L08.8        1\n",
       "N30          1\n",
       "N11          1\n",
       "K04.0        1\n",
       "H68.1        1\n",
       "H90.3        1\n",
       "M45          1\n",
       "M79.2        1\n",
       "M70.2        1\n",
       "H01.0        1\n",
       "Name: Код_диагноза, Length: 216, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['Код_диагноза'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit/bow_simple_nn2_diag_all_compl_uni-n_gram.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
