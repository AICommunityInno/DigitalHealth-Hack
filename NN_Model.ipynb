{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, InputLayer\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy\n",
    "\n",
    "from os.path import join, basename, exists\n",
    "from os import makedirs, listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_target_train(data, reduce_classes=False):\n",
    "    diagnoses = data['Код_диагноза'].copy()\n",
    "    \n",
    "    if reduce_classes:\n",
    "        pop_diagnoses = set(utils.get_most_popular_diagnoses(diagnoses, percent=.80))\n",
    "        most_pop_diagnose = scipy.stats.mode(diagnoses)[0][0]\n",
    "    else:\n",
    "        pop_diagnoses = set(diagnoses)\n",
    "        most_pop_diagnose = scipy.stats.mode(diagnoses)[0][0]\n",
    "    \n",
    "    diagnoses = diagnoses.apply(\n",
    "        lambda diag: diag if diag in pop_diagnoses else most_pop_diagnose\n",
    "    )\n",
    "    \n",
    "    return diagnoses, pop_diagnoses, most_pop_diagnose\n",
    "\n",
    "def preproc_target_test(data, pop_diagnoses, most_pop_diagnose):\n",
    "    diagnoses = data['Код_диагноза'].copy()\n",
    "    \n",
    "    diagnoses = diagnoses.apply(\n",
    "        lambda diag: diag if diag in pop_diagnoses else most_pop_diagnose\n",
    "    )\n",
    "    \n",
    "    return diagnoses, pop_diagnoses, most_pop_diagnose\n",
    "\n",
    "def join_topics(data, topics):\n",
    "    data = data.copy()\n",
    "    \n",
    "    topics_df = pd.DataFrame(dict(zip(\n",
    "        ['topic' + str(i) for i in range(topics.shape[1])],\n",
    "        [topics[:, i] for i in range(topics.shape[1])])))\n",
    "    topics_df['Id_Записи'] = data['Id_Записи']\n",
    "\n",
    "    data = data.join(topics_df, on='Id_Записи', rsuffix='_topics', how='outer')\n",
    "    data = data.drop(columns=['Id_Записи_topics'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class DoctorsPopularityTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        doctors = x.fillna('sss')\n",
    "        doctors_voc, counts = np.unique(doctors, return_counts=True)\n",
    "        self.pop_doctor = doctors_voc[np.argsort(counts)[::-1][0]]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = x.fillna('sss')\n",
    "        x[x == 'sss'] = self.pop_doctor\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GenderTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        x[x == 1] = 0\n",
    "        x[x == 2] = 1\n",
    "        \n",
    "        return np.expand_dims(x, axis=1)\n",
    "    \n",
    "class AgeTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return np.expand_dims(x, axis=1)\n",
    "    \n",
    "class TopicsTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x\n",
    "\n",
    "class ClinicTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        clinics, counts = np.unique(x, return_counts=True)\n",
    "        self.clinics_to_idx = dict(zip(clinics, range(len(clinics))))\n",
    "        \n",
    "        self.most_pop_clinic_idx = np.argmax(counts)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x_vec = np.zeros((x.shape[0], len(self.clinics_to_idx)), dtype=np.float32)\n",
    "        for i, clinic in enumerate(x):\n",
    "            if clinic in self.clinics_to_idx:\n",
    "                x_vec[i, self.clinics_to_idx[clinic]] = 1.\n",
    "            else:\n",
    "                x_vec[i, self.most_pop_clinic_idx] = 1.\n",
    "        \n",
    "        return x_vec\n",
    "    \n",
    "class RepeatsTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return np.expand_dims(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, batch_size, epochs, tb_log_dir, model_path, validation_split=0.3):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.tb_log_dir = tb_log_dir\n",
    "        self.model_path = model_path\n",
    "        self.validation_split = validation_split\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        self.classes = np.unique(y)\n",
    "        self.classes_voc = dict(zip(self.classes, range(self.classes.shape[0])))\n",
    "        self.voc_classes = dict(zip(range(self.classes.shape[0]), self.classes))\n",
    "        y_proc = np.zeros((y.shape[0], self.classes.shape[0]), dtype=np.float32)\n",
    "        for i, yc in enumerate(y):\n",
    "            y_proc[i, self.classes_voc[yc]] = 1.\n",
    "        \n",
    "        self.model = Sequential([\n",
    "            InputLayer(input_shape=(x.shape[1],)),\n",
    "            Dense(2048, activation='sigmoid'),\n",
    "            Dense(2048, activation='sigmoid'),\n",
    "#             Dense(4096, activation='sigmoid'),\n",
    "#             Dense(4096, activation='sigmoid'),\n",
    "            Dense(self.classes.shape[0], activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        optim = RMSprop(lr=1e-4, decay=1e-6)\n",
    "        self.model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model.fit(x, y_proc,\n",
    "                  batch_size=self.batch_size, epochs=self.epochs,\n",
    "                  callbacks=[\n",
    "                      TensorBoard(log_dir=self.tb_log_dir, batch_size=self.batch_size),\n",
    "                      ModelCheckpoint(filepath=self.model_path, monitor='acc', period=5)\n",
    "                  ],\n",
    "                  validation_split=self.validation_split)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, x):\n",
    "        pred = self.model.predict(x)\n",
    "        max_idxs = np.argmax(pred, axis=1)\n",
    "        \n",
    "        return np.array(list(map(lambda max_idx: self.voc_classes[max_idx], max_idxs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_validation = False\n",
    "validation_split = 0.3\n",
    "\n",
    "experiment_dir = 'simple_models'\n",
    "model_name = 'nn_3dense' + str(utils.get_next_model_id(experiment_dir))\n",
    "tb_log_dir = join(experiment_dir, 'log', model_name)\n",
    "models_dir = join(experiment_dir, 'models')\n",
    "\n",
    "if no_validation:\n",
    "    model_path = utils.get_model_fname_pattern(models_dir, model_name, no_validation=True)\n",
    "    validation_split = 0.0\n",
    "else:\n",
    "    model_path = utils.get_model_fname_pattern(models_dir, model_name, no_validation=False)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('complaints_pipe', Pipeline([\n",
    "                ('complaint_selector', ItemSelector(key='Жалобы (unigramm)')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,1), min_df=10, stop_words=stopwords.words('russian')))\n",
    "            ])),\n",
    "            ('complaints_n_pipe', Pipeline([\n",
    "                ('complaint_n_selector', ItemSelector(key='Жалобы (ngramm)')),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,1), min_df=1, stop_words=stopwords.words('russian')))\n",
    "            ])),\n",
    "            ('doctor_pipe', Pipeline([\n",
    "                ('doctor_selector', ItemSelector(key='Врач')),\n",
    "                ('doc_pop', DoctorsPopularityTransformator()),\n",
    "                ('count_vect', CountVectorizer())\n",
    "            ])),\n",
    "            ('gender_pipe', Pipeline([\n",
    "                ('gender_selector', ItemSelector(key='Пол')),\n",
    "                ('gender_transform', GenderTransformator())\n",
    "            ])),\n",
    "            ('age_pipe', Pipeline([\n",
    "                ('age_selector', ItemSelector(key='Возраст')),\n",
    "                ('age_transformator', AgeTransformator())\n",
    "            ])),\n",
    "            ('topics_pipe', Pipeline([\n",
    "                ('topic_selector', ItemSelector(key=['topic' + str(i) for i in range(355)])),\n",
    "                ('topics_transform', TopicsTransformator())\n",
    "            ])),\n",
    "            ('clinic_pipe', Pipeline([\n",
    "                ('clinic_selector', ItemSelector(key='Клиника')),\n",
    "                ('clinic_transform', ClinicTransformator())\n",
    "            ])),\n",
    "            ('repeats_pipe', Pipeline([\n",
    "                ('repeats_selector', ItemSelector(key='Повторный приём')),\n",
    "                ('repeats_transform', RepeatsTransformator())\n",
    "            ]))\n",
    "        ]\n",
    "    )),\n",
    "    ('clf', NNModel(batch_size=128, epochs=200, tb_log_dir=tb_log_dir,\n",
    "                    model_path=model_path, validation_split=validation_split))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = utils.load_data('data/train_data_complaints_repeats_doctors.csv')\n",
    "train_topics = np.load('data/topics_train_ngramm.npy')\n",
    "train = join_topics(train, train_topics)\n",
    "# train, valid = train_test_split(train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train_y, pop_diagnoses, most_pop_diagnose = preproc_target_train(train, reduce_classes=False)\n",
    "# valid_y, _, _ = preproc_target_test(valid, pop_diagnoses, most_pop_diagnose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2302,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43383 samples, validate on 18593 samples\n",
      "Epoch 1/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 5.6875 - acc: 0.0505 - val_loss: 6.1644 - val_acc: 0.0463\n",
      "Epoch 2/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 5.4402 - acc: 0.0767 - val_loss: 6.0659 - val_acc: 0.0805\n",
      "Epoch 3/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 5.1680 - acc: 0.1007 - val_loss: 5.7823 - val_acc: 0.1002\n",
      "Epoch 4/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 4.8725 - acc: 0.1392 - val_loss: 5.5801 - val_acc: 0.1458\n",
      "Epoch 5/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 4.6273 - acc: 0.1709 - val_loss: 5.3343 - val_acc: 0.1569\n",
      "Epoch 6/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 4.3945 - acc: 0.1871 - val_loss: 5.1719 - val_acc: 0.1717\n",
      "Epoch 7/200\n",
      "43383/43383 [==============================] - 8s 182us/step - loss: 4.1879 - acc: 0.2124 - val_loss: 4.9955 - val_acc: 0.1986\n",
      "Epoch 8/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 4.0039 - acc: 0.2283 - val_loss: 4.8409 - val_acc: 0.2052\n",
      "Epoch 9/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 3.8494 - acc: 0.2400 - val_loss: 4.7017 - val_acc: 0.2136\n",
      "Epoch 10/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 3.7264 - acc: 0.2473 - val_loss: 4.6134 - val_acc: 0.2188\n",
      "Epoch 11/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 3.6314 - acc: 0.2530 - val_loss: 4.5705 - val_acc: 0.2260\n",
      "Epoch 12/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 3.5520 - acc: 0.2596 - val_loss: 4.5138 - val_acc: 0.2334\n",
      "Epoch 13/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.4868 - acc: 0.2650 - val_loss: 4.4570 - val_acc: 0.2359\n",
      "Epoch 14/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 3.4296 - acc: 0.2711 - val_loss: 4.4128 - val_acc: 0.2385\n",
      "Epoch 15/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.3787 - acc: 0.2734 - val_loss: 4.4294 - val_acc: 0.2403\n",
      "Epoch 16/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.3326 - acc: 0.2779 - val_loss: 4.3414 - val_acc: 0.2444\n",
      "Epoch 17/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 3.2912 - acc: 0.2841 - val_loss: 4.2870 - val_acc: 0.2501\n",
      "Epoch 18/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 3.2533 - acc: 0.2894 - val_loss: 4.2816 - val_acc: 0.2551\n",
      "Epoch 19/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 3.2173 - acc: 0.2927 - val_loss: 4.2236 - val_acc: 0.2584\n",
      "Epoch 20/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 3.1861 - acc: 0.2959 - val_loss: 4.1986 - val_acc: 0.2616\n",
      "Epoch 21/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.1571 - acc: 0.3008 - val_loss: 4.1545 - val_acc: 0.2625\n",
      "Epoch 22/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 3.1302 - acc: 0.3020 - val_loss: 4.1312 - val_acc: 0.2695\n",
      "Epoch 23/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.1050 - acc: 0.3057 - val_loss: 4.0862 - val_acc: 0.2713\n",
      "Epoch 24/200\n",
      "43383/43383 [==============================] - 8s 181us/step - loss: 3.0812 - acc: 0.3079 - val_loss: 4.1149 - val_acc: 0.2674\n",
      "Epoch 25/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.0592 - acc: 0.3122 - val_loss: 4.0805 - val_acc: 0.2723\n",
      "Epoch 26/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 3.0382 - acc: 0.3129 - val_loss: 4.0408 - val_acc: 0.2757\n",
      "Epoch 27/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.0184 - acc: 0.3154 - val_loss: 4.0432 - val_acc: 0.2780\n",
      "Epoch 28/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 3.0000 - acc: 0.3181 - val_loss: 4.0321 - val_acc: 0.2777\n",
      "Epoch 29/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.9821 - acc: 0.3218 - val_loss: 4.0182 - val_acc: 0.2809\n",
      "Epoch 30/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.9658 - acc: 0.3234 - val_loss: 4.0137 - val_acc: 0.2815\n",
      "Epoch 31/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.9515 - acc: 0.3241 - val_loss: 3.9998 - val_acc: 0.2811\n",
      "Epoch 32/200\n",
      "43383/43383 [==============================] - 8s 182us/step - loss: 2.9363 - acc: 0.3264 - val_loss: 4.0006 - val_acc: 0.2841\n",
      "Epoch 33/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.9231 - acc: 0.3276 - val_loss: 3.9898 - val_acc: 0.2870\n",
      "Epoch 34/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.9105 - acc: 0.3305 - val_loss: 3.9877 - val_acc: 0.2872\n",
      "Epoch 35/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.8972 - acc: 0.3311 - val_loss: 3.9565 - val_acc: 0.2865\n",
      "Epoch 36/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.8853 - acc: 0.3341 - val_loss: 3.9693 - val_acc: 0.2872\n",
      "Epoch 37/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.8737 - acc: 0.3339 - val_loss: 3.9987 - val_acc: 0.2895\n",
      "Epoch 38/200\n",
      "43383/43383 [==============================] - 8s 183us/step - loss: 2.8625 - acc: 0.3361 - val_loss: 3.9490 - val_acc: 0.2932\n",
      "Epoch 39/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.8517 - acc: 0.3383 - val_loss: 3.9513 - val_acc: 0.2908\n",
      "Epoch 40/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.8423 - acc: 0.3392 - val_loss: 3.9377 - val_acc: 0.2928\n",
      "Epoch 41/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.8292 - acc: 0.3400 - val_loss: 3.8961 - val_acc: 0.2945\n",
      "Epoch 42/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.8181 - acc: 0.3427 - val_loss: 3.9214 - val_acc: 0.2933\n",
      "Epoch 43/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.8100 - acc: 0.3436 - val_loss: 3.9067 - val_acc: 0.2967\n",
      "Epoch 44/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.7978 - acc: 0.3438 - val_loss: 3.8669 - val_acc: 0.2948\n",
      "Epoch 45/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.7890 - acc: 0.3452 - val_loss: 3.8910 - val_acc: 0.2967\n",
      "Epoch 46/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.7805 - acc: 0.3463 - val_loss: 3.8680 - val_acc: 0.2970\n",
      "Epoch 47/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.7672 - acc: 0.3468 - val_loss: 3.8940 - val_acc: 0.2973\n",
      "Epoch 48/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7558 - acc: 0.3476 - val_loss: 3.8694 - val_acc: 0.2997\n",
      "Epoch 49/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7481 - acc: 0.3490 - val_loss: 3.8508 - val_acc: 0.3006\n",
      "Epoch 50/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.7425 - acc: 0.3500 - val_loss: 3.9018 - val_acc: 0.2989\n",
      "Epoch 51/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7379 - acc: 0.3512 - val_loss: 3.8803 - val_acc: 0.2984\n",
      "Epoch 52/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.7323 - acc: 0.3515 - val_loss: 3.8654 - val_acc: 0.2995\n",
      "Epoch 53/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.7268 - acc: 0.3532 - val_loss: 3.8526 - val_acc: 0.3019\n",
      "Epoch 54/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.7204 - acc: 0.3530 - val_loss: 3.8817 - val_acc: 0.3030\n",
      "Epoch 55/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7107 - acc: 0.3546 - val_loss: 3.8997 - val_acc: 0.2990\n",
      "Epoch 56/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.7064 - acc: 0.3553 - val_loss: 3.8708 - val_acc: 0.3002\n",
      "Epoch 57/200\n",
      "43383/43383 [==============================] - 8s 182us/step - loss: 2.6990 - acc: 0.3551 - val_loss: 3.8387 - val_acc: 0.3027\n",
      "Epoch 58/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6957 - acc: 0.3569 - val_loss: 3.8640 - val_acc: 0.3018\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.6905 - acc: 0.3574 - val_loss: 3.8587 - val_acc: 0.3050\n",
      "Epoch 60/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6857 - acc: 0.3584 - val_loss: 3.8553 - val_acc: 0.3037\n",
      "Epoch 61/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6794 - acc: 0.3586 - val_loss: 3.8560 - val_acc: 0.3047\n",
      "Epoch 62/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6740 - acc: 0.3604 - val_loss: 3.8603 - val_acc: 0.3022\n",
      "Epoch 63/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6688 - acc: 0.3608 - val_loss: 3.8422 - val_acc: 0.3060\n",
      "Epoch 64/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6618 - acc: 0.3613 - val_loss: 3.8581 - val_acc: 0.3038\n",
      "Epoch 65/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.6559 - acc: 0.3627 - val_loss: 3.8319 - val_acc: 0.3042\n",
      "Epoch 66/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.6473 - acc: 0.3637 - val_loss: 3.8098 - val_acc: 0.3070\n",
      "Epoch 67/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6451 - acc: 0.3644 - val_loss: 3.8394 - val_acc: 0.3065\n",
      "Epoch 68/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.6418 - acc: 0.3648 - val_loss: 3.8423 - val_acc: 0.3071\n",
      "Epoch 69/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6364 - acc: 0.3654 - val_loss: 3.8342 - val_acc: 0.3084\n",
      "Epoch 70/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.6262 - acc: 0.3674 - val_loss: 3.8235 - val_acc: 0.3081\n",
      "Epoch 71/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.6207 - acc: 0.3668 - val_loss: 3.7716 - val_acc: 0.3082\n",
      "Epoch 72/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6116 - acc: 0.3682 - val_loss: 3.7602 - val_acc: 0.3109\n",
      "Epoch 73/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.6048 - acc: 0.3687 - val_loss: 3.7553 - val_acc: 0.3103\n",
      "Epoch 74/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.5982 - acc: 0.3687 - val_loss: 3.7449 - val_acc: 0.3090\n",
      "Epoch 75/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.5958 - acc: 0.3693 - val_loss: 3.7922 - val_acc: 0.3094\n",
      "Epoch 76/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.5880 - acc: 0.3708 - val_loss: 3.7891 - val_acc: 0.3110\n",
      "Epoch 77/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5874 - acc: 0.3710 - val_loss: 3.8145 - val_acc: 0.3103\n",
      "Epoch 78/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5859 - acc: 0.3719 - val_loss: 3.7881 - val_acc: 0.3105\n",
      "Epoch 79/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.5842 - acc: 0.3732 - val_loss: 3.8143 - val_acc: 0.3096\n",
      "Epoch 80/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.5782 - acc: 0.3738 - val_loss: 3.8286 - val_acc: 0.3110\n",
      "Epoch 81/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5767 - acc: 0.3745 - val_loss: 3.8367 - val_acc: 0.3122\n",
      "Epoch 82/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.5754 - acc: 0.3741 - val_loss: 3.8414 - val_acc: 0.3135\n",
      "Epoch 83/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5734 - acc: 0.3754 - val_loss: 3.8001 - val_acc: 0.3103\n",
      "Epoch 84/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5615 - acc: 0.3769 - val_loss: 3.7765 - val_acc: 0.3136\n",
      "Epoch 85/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5485 - acc: 0.3765 - val_loss: 3.8156 - val_acc: 0.3116\n",
      "Epoch 86/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5482 - acc: 0.3784 - val_loss: 3.8088 - val_acc: 0.3133\n",
      "Epoch 87/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.5503 - acc: 0.3777 - val_loss: 3.8322 - val_acc: 0.3118\n",
      "Epoch 88/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5512 - acc: 0.3791 - val_loss: 3.8162 - val_acc: 0.3118\n",
      "Epoch 89/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5437 - acc: 0.3798 - val_loss: 3.8292 - val_acc: 0.3150\n",
      "Epoch 90/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5369 - acc: 0.3811 - val_loss: 3.8438 - val_acc: 0.3135\n",
      "Epoch 91/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.5390 - acc: 0.3802 - val_loss: 3.8436 - val_acc: 0.3146\n",
      "Epoch 92/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.5359 - acc: 0.3802 - val_loss: 3.8167 - val_acc: 0.3148\n",
      "Epoch 93/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.5296 - acc: 0.3822 - val_loss: 3.8340 - val_acc: 0.3143\n",
      "Epoch 94/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.5271 - acc: 0.3817 - val_loss: 3.8397 - val_acc: 0.3158\n",
      "Epoch 95/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.5258 - acc: 0.3826 - val_loss: 3.8371 - val_acc: 0.3166\n",
      "Epoch 96/200\n",
      "43383/43383 [==============================] - 8s 184us/step - loss: 2.5185 - acc: 0.3842 - val_loss: 3.7931 - val_acc: 0.3141\n",
      "Epoch 97/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5077 - acc: 0.3839 - val_loss: 3.8281 - val_acc: 0.3160\n",
      "Epoch 98/200\n",
      "43383/43383 [==============================] - 8s 183us/step - loss: 2.5077 - acc: 0.3849 - val_loss: 3.8506 - val_acc: 0.3155\n",
      "Epoch 99/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.5081 - acc: 0.3853 - val_loss: 3.8480 - val_acc: 0.3164\n",
      "Epoch 100/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.5006 - acc: 0.3856 - val_loss: 3.8244 - val_acc: 0.3174\n",
      "Epoch 101/200\n",
      "43383/43383 [==============================] - 8s 182us/step - loss: 2.4962 - acc: 0.3862 - val_loss: 3.7853 - val_acc: 0.3156\n",
      "Epoch 102/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.4932 - acc: 0.3868 - val_loss: 3.8343 - val_acc: 0.3174\n",
      "Epoch 103/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.4986 - acc: 0.3879 - val_loss: 3.8252 - val_acc: 0.3175\n",
      "Epoch 104/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.4999 - acc: 0.3888 - val_loss: 3.8646 - val_acc: 0.3177\n",
      "Epoch 105/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.4993 - acc: 0.3885 - val_loss: 3.7907 - val_acc: 0.3164\n",
      "Epoch 106/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.4907 - acc: 0.3890 - val_loss: 3.8367 - val_acc: 0.3178\n",
      "Epoch 107/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4820 - acc: 0.3899 - val_loss: 3.8068 - val_acc: 0.3174\n",
      "Epoch 108/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.4759 - acc: 0.3903 - val_loss: 3.7684 - val_acc: 0.3175\n",
      "Epoch 109/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4641 - acc: 0.3910 - val_loss: 3.7311 - val_acc: 0.3185\n",
      "Epoch 110/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.4592 - acc: 0.3912 - val_loss: 3.7731 - val_acc: 0.3201\n",
      "Epoch 111/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.4640 - acc: 0.3916 - val_loss: 3.8055 - val_acc: 0.3180\n",
      "Epoch 112/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.4672 - acc: 0.3920 - val_loss: 3.8088 - val_acc: 0.3197\n",
      "Epoch 113/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4531 - acc: 0.3927 - val_loss: 3.7500 - val_acc: 0.3185\n",
      "Epoch 114/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.4516 - acc: 0.3942 - val_loss: 3.8078 - val_acc: 0.3191\n",
      "Epoch 115/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.4440 - acc: 0.3942 - val_loss: 3.7994 - val_acc: 0.3190\n",
      "Epoch 116/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.4429 - acc: 0.3949 - val_loss: 3.8137 - val_acc: 0.3201\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4423 - acc: 0.3965 - val_loss: 3.7873 - val_acc: 0.3207\n",
      "Epoch 118/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4253 - acc: 0.3980 - val_loss: 3.7249 - val_acc: 0.3190\n",
      "Epoch 119/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4214 - acc: 0.3965 - val_loss: 3.8122 - val_acc: 0.3216\n",
      "Epoch 120/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4260 - acc: 0.3984 - val_loss: 3.7882 - val_acc: 0.3187\n",
      "Epoch 121/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.4259 - acc: 0.3979 - val_loss: 3.8415 - val_acc: 0.3211\n",
      "Epoch 122/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4356 - acc: 0.3970 - val_loss: 3.8585 - val_acc: 0.3209\n",
      "Epoch 123/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4384 - acc: 0.3994 - val_loss: 3.8736 - val_acc: 0.3211\n",
      "Epoch 124/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4390 - acc: 0.3983 - val_loss: 3.8904 - val_acc: 0.3214\n",
      "Epoch 125/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4400 - acc: 0.3983 - val_loss: 3.8762 - val_acc: 0.3206\n",
      "Epoch 126/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4443 - acc: 0.3991 - val_loss: 3.8815 - val_acc: 0.3208\n",
      "Epoch 127/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.4434 - acc: 0.3994 - val_loss: 3.8878 - val_acc: 0.3219\n",
      "Epoch 128/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.4423 - acc: 0.4002 - val_loss: 3.8670 - val_acc: 0.3214\n",
      "Epoch 129/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.4412 - acc: 0.3993 - val_loss: 3.8912 - val_acc: 0.3228\n",
      "Epoch 130/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.4435 - acc: 0.3995 - val_loss: 3.9053 - val_acc: 0.3216\n",
      "Epoch 131/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4371 - acc: 0.4015 - val_loss: 3.9078 - val_acc: 0.3200\n",
      "Epoch 132/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4417 - acc: 0.4023 - val_loss: 3.9306 - val_acc: 0.3222\n",
      "Epoch 133/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4332 - acc: 0.4018 - val_loss: 3.8668 - val_acc: 0.3215\n",
      "Epoch 134/200\n",
      "43383/43383 [==============================] - 8s 183us/step - loss: 2.4171 - acc: 0.4038 - val_loss: 3.9029 - val_acc: 0.3224\n",
      "Epoch 135/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4200 - acc: 0.4041 - val_loss: 3.8651 - val_acc: 0.3204\n",
      "Epoch 136/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.4140 - acc: 0.4041 - val_loss: 3.8939 - val_acc: 0.3226\n",
      "Epoch 137/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4116 - acc: 0.4038 - val_loss: 3.8699 - val_acc: 0.3239\n",
      "Epoch 138/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.4084 - acc: 0.4047 - val_loss: 3.8786 - val_acc: 0.3235\n",
      "Epoch 139/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.4070 - acc: 0.4073 - val_loss: 3.8630 - val_acc: 0.3243\n",
      "Epoch 140/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.3958 - acc: 0.4057 - val_loss: 3.8502 - val_acc: 0.3226\n",
      "Epoch 141/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3991 - acc: 0.4076 - val_loss: 3.8285 - val_acc: 0.3221\n",
      "Epoch 142/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.3886 - acc: 0.4089 - val_loss: 3.8461 - val_acc: 0.3250\n",
      "Epoch 143/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3883 - acc: 0.4089 - val_loss: 3.8583 - val_acc: 0.3241\n",
      "Epoch 144/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3931 - acc: 0.4093 - val_loss: 3.8824 - val_acc: 0.3239\n",
      "Epoch 145/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.3929 - acc: 0.4088 - val_loss: 3.8661 - val_acc: 0.3259\n",
      "Epoch 146/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.3858 - acc: 0.4092 - val_loss: 3.8777 - val_acc: 0.3237\n",
      "Epoch 147/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3905 - acc: 0.4089 - val_loss: 3.8663 - val_acc: 0.3230\n",
      "Epoch 148/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3863 - acc: 0.4099 - val_loss: 3.8755 - val_acc: 0.3257\n",
      "Epoch 149/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3845 - acc: 0.4102 - val_loss: 3.8207 - val_acc: 0.3247\n",
      "Epoch 150/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3651 - acc: 0.4110 - val_loss: 3.8063 - val_acc: 0.3235\n",
      "Epoch 151/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3664 - acc: 0.4112 - val_loss: 3.8139 - val_acc: 0.3249\n",
      "Epoch 152/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.3597 - acc: 0.4122 - val_loss: 3.8689 - val_acc: 0.3228\n",
      "Epoch 153/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3639 - acc: 0.4119 - val_loss: 3.8689 - val_acc: 0.3249\n",
      "Epoch 154/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3643 - acc: 0.4135 - val_loss: 3.8847 - val_acc: 0.3230\n",
      "Epoch 155/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3549 - acc: 0.4118 - val_loss: 3.8195 - val_acc: 0.3267\n",
      "Epoch 156/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3497 - acc: 0.4126 - val_loss: 3.8273 - val_acc: 0.3250\n",
      "Epoch 157/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3373 - acc: 0.4135 - val_loss: 3.7999 - val_acc: 0.3251\n",
      "Epoch 158/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3464 - acc: 0.4136 - val_loss: 3.8828 - val_acc: 0.3253\n",
      "Epoch 159/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3615 - acc: 0.4146 - val_loss: 3.8978 - val_acc: 0.3225\n",
      "Epoch 160/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3576 - acc: 0.4144 - val_loss: 3.8879 - val_acc: 0.3229\n",
      "Epoch 161/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.3632 - acc: 0.4145 - val_loss: 3.8904 - val_acc: 0.3255\n",
      "Epoch 162/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3609 - acc: 0.4168 - val_loss: 3.9117 - val_acc: 0.3250\n",
      "Epoch 163/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3634 - acc: 0.4160 - val_loss: 3.8875 - val_acc: 0.3244\n",
      "Epoch 164/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.3505 - acc: 0.4155 - val_loss: 3.8856 - val_acc: 0.3216\n",
      "Epoch 165/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3370 - acc: 0.4183 - val_loss: 3.8697 - val_acc: 0.3256\n",
      "Epoch 166/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.3390 - acc: 0.4179 - val_loss: 3.9014 - val_acc: 0.3250\n",
      "Epoch 167/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3350 - acc: 0.4199 - val_loss: 3.8464 - val_acc: 0.3251\n",
      "Epoch 168/200\n",
      "43383/43383 [==============================] - 8s 182us/step - loss: 2.3214 - acc: 0.4199 - val_loss: 3.8082 - val_acc: 0.3276\n",
      "Epoch 169/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3111 - acc: 0.4198 - val_loss: 3.8357 - val_acc: 0.3271\n",
      "Epoch 170/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3036 - acc: 0.4207 - val_loss: 3.8409 - val_acc: 0.3279\n",
      "Epoch 171/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.2970 - acc: 0.4217 - val_loss: 3.8190 - val_acc: 0.3248\n",
      "Epoch 172/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3012 - acc: 0.4225 - val_loss: 3.8912 - val_acc: 0.3227\n",
      "Epoch 173/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.3136 - acc: 0.4221 - val_loss: 3.8991 - val_acc: 0.3254\n",
      "Epoch 174/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3276 - acc: 0.4208 - val_loss: 3.9009 - val_acc: 0.3233\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.3337 - acc: 0.4213 - val_loss: 3.9044 - val_acc: 0.3225\n",
      "Epoch 176/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3261 - acc: 0.4233 - val_loss: 3.9352 - val_acc: 0.3253\n",
      "Epoch 177/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3349 - acc: 0.4232 - val_loss: 3.9455 - val_acc: 0.3252\n",
      "Epoch 178/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.3195 - acc: 0.4232 - val_loss: 3.9139 - val_acc: 0.3256\n",
      "Epoch 179/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.3201 - acc: 0.4255 - val_loss: 3.9014 - val_acc: 0.3260\n",
      "Epoch 180/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3114 - acc: 0.4242 - val_loss: 3.8914 - val_acc: 0.3226\n",
      "Epoch 181/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3116 - acc: 0.4256 - val_loss: 3.9229 - val_acc: 0.3236\n",
      "Epoch 182/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3170 - acc: 0.4236 - val_loss: 3.8739 - val_acc: 0.3231\n",
      "Epoch 183/200\n",
      "43383/43383 [==============================] - 8s 179us/step - loss: 2.3040 - acc: 0.4252 - val_loss: 3.8871 - val_acc: 0.3225\n",
      "Epoch 184/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.3088 - acc: 0.4261 - val_loss: 3.9234 - val_acc: 0.3244\n",
      "Epoch 185/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.2908 - acc: 0.4256 - val_loss: 3.8599 - val_acc: 0.3262\n",
      "Epoch 186/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.2875 - acc: 0.4264 - val_loss: 3.8874 - val_acc: 0.3242\n",
      "Epoch 187/200\n",
      "43383/43383 [==============================] - 8s 173us/step - loss: 2.2982 - acc: 0.4268 - val_loss: 3.9217 - val_acc: 0.3251\n",
      "Epoch 188/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.3001 - acc: 0.4264 - val_loss: 3.9021 - val_acc: 0.3274\n",
      "Epoch 189/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.2897 - acc: 0.4293 - val_loss: 3.9093 - val_acc: 0.3259\n",
      "Epoch 190/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.2955 - acc: 0.4288 - val_loss: 3.8983 - val_acc: 0.3272\n",
      "Epoch 191/200\n",
      "43383/43383 [==============================] - 8s 180us/step - loss: 2.2830 - acc: 0.4293 - val_loss: 3.9082 - val_acc: 0.3254\n",
      "Epoch 192/200\n",
      "43383/43383 [==============================] - 8s 177us/step - loss: 2.2893 - acc: 0.4291 - val_loss: 3.9499 - val_acc: 0.3260\n",
      "Epoch 193/200\n",
      "43383/43383 [==============================] - 8s 178us/step - loss: 2.3008 - acc: 0.4309 - val_loss: 3.9118 - val_acc: 0.3269\n",
      "Epoch 194/200\n",
      "43383/43383 [==============================] - 8s 182us/step - loss: 2.2789 - acc: 0.4309 - val_loss: 3.8746 - val_acc: 0.3275\n",
      "Epoch 195/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.2736 - acc: 0.4318 - val_loss: 3.8999 - val_acc: 0.3244\n",
      "Epoch 196/200\n",
      "43383/43383 [==============================] - 8s 174us/step - loss: 2.2749 - acc: 0.4318 - val_loss: 3.8866 - val_acc: 0.3246\n",
      "Epoch 197/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.2616 - acc: 0.4305 - val_loss: 3.8745 - val_acc: 0.3240\n",
      "Epoch 198/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.2651 - acc: 0.4329 - val_loss: 3.8839 - val_acc: 0.3258\n",
      "Epoch 199/200\n",
      "43383/43383 [==============================] - 8s 176us/step - loss: 2.2737 - acc: 0.4322 - val_loss: 3.8809 - val_acc: 0.3253\n",
      "Epoch 200/200\n",
      "43383/43383 [==============================] - 8s 175us/step - loss: 2.2542 - acc: 0.4349 - val_loss: 3.8999 - val_acc: 0.3237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('complaints_pipe', Pipeline(memory=None,\n",
       "     steps=[('complaint_selector', ItemSelector(key='Жалобы (unigramm)')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, e...poch:02d}_{val_acc:.2f}.h5',\n",
       "    tb_log_dir='simple_models/log/nn_3dense33', validation_split=0.3))])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipe.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A46', 'A63.0', 'B00', 'B00.1', 'B00.8', 'B02', 'B07', 'B08.1',\n",
       "        'B35.1', 'B35.3', 'B35.6', 'B36.0', 'B37', 'B37.3+', 'B97.1',\n",
       "        'B97.7', 'D17.0', 'D17.1', 'D17.2', 'D18.0', 'D23', 'D23.3',\n",
       "        'D23.5', 'D23.9', 'D24', 'D25', 'D50', 'D50.9', 'D89.8', 'E01.1',\n",
       "        'E01.8', 'E03.8', 'E03.9', 'E04.1', 'E04.2', 'E05.0', 'E06.3',\n",
       "        'E11', 'E22.1', 'E28', 'E28.1', 'E28.8', 'E28.9', 'E66.0', 'E89.0',\n",
       "        'F41.0', 'F41.2', 'F45.3', 'F95.0', 'G24', 'G24.9', 'G43.0',\n",
       "        'G44.2', 'G50.0', 'G51.0', 'G55*', 'G55.1*', 'G55.3*', 'G56.0',\n",
       "        'G56.2', 'G90', 'G90.8', 'G90.9', 'G93.4', 'H00', 'H00.0', 'H00.1',\n",
       "        'H01.0', 'H04.1', 'H10.0', 'H10.1', 'H10.2', 'H10.3', 'H10.5',\n",
       "        'H10.8', 'H11.3', 'H15.1', 'H16.1', 'H16.2', 'H20.0', 'H35.0',\n",
       "        'H35.3', 'H35.4', 'H40.1', 'H52.0', 'H52.1', 'H52.2', 'H52.4',\n",
       "        'H52.5', 'H60', 'H61.2', 'H65', 'H65.0', 'H65.1', 'H66.0', 'H66.1',\n",
       "        'H68.0', 'H68.1', 'H81.1', 'H81.2', 'H90.3', 'H93.1', 'I10', 'I11',\n",
       "        'I11.9', 'I20', 'I20.1', 'I20.8', 'I20.9', 'I25', 'I25.0', 'I25.1',\n",
       "        'I34.1', 'I47', 'I48', 'I49.8', 'I67.2', 'I67.4', 'I67.8', 'I70.2',\n",
       "        'I83', 'I83.9', 'I84.3', 'I84.4', 'I84.5', 'I86.1', 'I87.0',\n",
       "        'I87.2', 'J00', 'J01.0', 'J01.1', 'J01.8', 'J02', 'J02.9', 'J03',\n",
       "        'J03.9', 'J04.0', 'J04.1', 'J06.0', 'J06.9', 'J18', 'J18.0',\n",
       "        'J18.9', 'J20', 'J20.9', 'J30', 'J30.0', 'J30.1', 'J30.3', 'J31.0',\n",
       "        'J31.1', 'J31.2', 'J34.2', 'J35.0', 'J35.2', 'J35.3', 'J35.8',\n",
       "        'J41.0', 'J45.0', 'J45.8', 'J45.9', 'K02.1', 'K04.0', 'K05.1',\n",
       "        'K05.3', 'K21', 'K21.0', 'K21.9', 'K29.3', 'K29.5', 'K29.9', 'K30',\n",
       "        'K40', 'K40.9', 'K43', 'K46', 'K52.9', 'K58', 'K58.0', 'K80.1',\n",
       "        'K81', 'K82.8', 'K83.9', 'K86.1', 'K87*', 'L01.0', 'L02.0',\n",
       "        'L02.2', 'L02.4', 'L04.0', 'L08.0', 'L08.8', 'L20', 'L20.8', 'L21',\n",
       "        'L23', 'L23.9', 'L28.0', 'L30.0', 'L30.2', 'L30.3', 'L30.8',\n",
       "        'L30.9', 'L40.0', 'L52', 'L60.0', 'L60.3', 'L63', 'L65.0', 'L70.0',\n",
       "        'L71', 'L71.0', 'L72', 'L73.2', 'L84', 'M06.0', 'M12.5', 'M15-M19',\n",
       "        'M15.8', 'M17', 'M17.1', 'M17.4', 'M20-M25', 'M21.4', 'M22.4',\n",
       "        'M23.2', 'M41', 'M41.1', 'M42', 'M42.1', 'M47', 'M47.8', 'M50.1',\n",
       "        'M51', 'M51.1', 'M51.3', 'M51.8', 'M53.0', 'M53.1', 'M53.8',\n",
       "        'M53.9', 'M54', 'M54.1', 'M54.2', 'M54.4', 'M54.5', 'M54.6',\n",
       "        'M54.8', 'M54.9', 'M65', 'M67.4', 'M70.2', 'M71.8', 'M72.5',\n",
       "        'M75.3', 'M76.0', 'M77.1', 'M77.3', 'M77.9', 'M79.1', 'M79.2',\n",
       "        'M79.6', 'N10', 'N11', 'N20', 'N20.0', 'N20.2', 'N30', 'N30.0',\n",
       "        'N30.2', 'N34', 'N34.1', 'N34.2', 'N40', 'N41.0', 'N41.1', 'N42',\n",
       "        'N42.8', 'N46', 'N47', 'N48.1', 'N48.6', 'N48.8', 'N60.1', 'N64',\n",
       "        'N64.4', 'N70', 'N72', 'N75.0', 'N76.0', 'N76.1', 'N77.1*',\n",
       "        'N80.0', 'N83.0', 'N83.2', 'N84.0', 'N85.0', 'N86', 'N94.0',\n",
       "        'N94.9', 'N95.2', 'O04', 'R52.0', 'S06.0', 'S20.2', 'S22.3',\n",
       "        'S52.5', 'S61.0', 'S62.3', 'S80', 'S80.0', 'S83.5', 'S92.3',\n",
       "        'S92.5', 'S93.4', 'T15.1', 'Z00', 'Z00.0', 'Z00.8', 'Z01.0',\n",
       "        'Z01.4', 'Z01.8', 'Z03.3', 'Z04.8', 'Z10.0', 'Z32.0', 'Z32.1',\n",
       "        'Z34.0', 'Z34.8', 'Z35', 'Z35.8'], dtype='<U7'),\n",
       " array([  49,    1,  133,    2,   17,   76,  342,   31,   70,   54,   51,\n",
       "         112,   12,  288,    1,   22,   21,    8,   33,    9,   36,    2,\n",
       "          45,  765,   39,  300,   19,  104,   18,   27,    2,  311,   41,\n",
       "          51,  687,   10,   74,  242,    7,  197,    1,   19,    3,   48,\n",
       "          19,   16,    2,   61,    8,   48,    9,    8,  253,   70,    5,\n",
       "          41,   22,   47,    2,   42,  442,   76,  114,   31,   27,   59,\n",
       "          19,   12,    5,   22,   29,  202,   14,   37,    1,    5,   16,\n",
       "           2,    2,    1,  303,   14,   27,    1,   53,  800,   66,  123,\n",
       "           9,  252,  686,   10,   10,    6,    7,   17,    6,    7,    4,\n",
       "           7,   14,   27,  168, 2224,   96,    2,    7,   80,   71,   12,\n",
       "          16,   16,   85,    9,   18,   73,  222,    6,  233,    9,   14,\n",
       "         490,   45,   65,   79,   10,    5,    2, 1297,  839,    2,   13,\n",
       "          36,   15,   19,   19,   49,  379,   28, 3165,  222,    8,   85,\n",
       "           6,   58,    2,   79,  122,   87,  100,  147,   96,   61, 1013,\n",
       "          15,    3,  159,   34,   34,   20,    3,   59,   26,   29,    5,\n",
       "         139,  273,    1,  153,  681, 1137, 1159,   22,   10,   17,    5,\n",
       "         114,  126,   50,   20,    8,   63,   80,  120,  394,    1,  112,\n",
       "         298,  134,   10,  113,   19,  161,  156,  335,    9,    4,   20,\n",
       "          29,  334,   25,  489,   68,    6,    3,  178,   23,   31,   84,\n",
       "         108,   98,  250,   70,    1,   61,    8,   16,   59,  298,   23,\n",
       "           1,   10,    7,  259,    2,  270,  101,    7,  151, 5466,   56,\n",
       "           4,    6,  158,   97,  109,  248,  148,   52,  436,   24,  173,\n",
       "           7,  399,  438,  335,  192, 1228,   46, 1640,   27,   77,   19,\n",
       "           8,    6,    2,   23,   23,    2,  184,   33,  129,   17,   53,\n",
       "           5,  250,    4,   25,  379,  157,   14,   43,  109,  584,    5,\n",
       "        2223,    1,  105,   12,    6,  130,   10,    5, 1399,    4,    5,\n",
       "          53,  388,    3, 2488, 1913, 1849,   53,  347,   13,  304,   19,\n",
       "         295,    8,   13,   69,   14,    4,    9,   26,    4,   36,   53,\n",
       "          23,    4,   17,   46,   11,    6,   33,   43,    5, 1376,  124,\n",
       "          72,  409, 1157,    1,    9,   89,   10, 3089,  103,   11,   20,\n",
       "          14]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train = utils.load_data('data/train_data_complaints_repeats_doctors.csv')\n",
    "train_topics = np.load('data/topics_train_ngramm.npy')\n",
    "train = join_topics(train, train_topics)\n",
    "train_y, pop_diagnoses, most_pop_diagnose = preproc_target_train(train, reduce_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "61976/61976 [==============================] - 13s 215us/step - loss: 5.8733 - acc: 0.0998\n",
      "Epoch 2/100\n",
      "61976/61976 [==============================] - 13s 212us/step - loss: 5.5176 - acc: 0.1808\n",
      "Epoch 3/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 5.1080 - acc: 0.2237\n",
      "Epoch 4/100\n",
      "61976/61976 [==============================] - 13s 212us/step - loss: 4.7338 - acc: 0.2444\n",
      "Epoch 5/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 4.4498 - acc: 0.2577\n",
      "Epoch 6/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 4.2481 - acc: 0.2679\n",
      "Epoch 7/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 4.1090 - acc: 0.2773\n",
      "Epoch 8/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 4.0043 - acc: 0.2860\n",
      "Epoch 9/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.9136 - acc: 0.2952\n",
      "Epoch 10/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.8513 - acc: 0.3007\n",
      "Epoch 11/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.7937 - acc: 0.3045\n",
      "Epoch 12/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.7485 - acc: 0.3090\n",
      "Epoch 13/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.7130 - acc: 0.3135\n",
      "Epoch 14/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.6764 - acc: 0.3183\n",
      "Epoch 15/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.6508 - acc: 0.3216\n",
      "Epoch 16/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.6254 - acc: 0.3243\n",
      "Epoch 17/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.6010 - acc: 0.3273\n",
      "Epoch 18/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.5754 - acc: 0.3303\n",
      "Epoch 19/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.5600 - acc: 0.3320\n",
      "Epoch 20/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.5423 - acc: 0.3338\n",
      "Epoch 21/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.5206 - acc: 0.3362\n",
      "Epoch 22/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.5016 - acc: 0.3372\n",
      "Epoch 23/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.4885 - acc: 0.3391\n",
      "Epoch 24/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4783 - acc: 0.3425\n",
      "Epoch 25/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4684 - acc: 0.3435\n",
      "Epoch 26/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4551 - acc: 0.3455\n",
      "Epoch 27/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.4406 - acc: 0.3463\n",
      "Epoch 28/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.4187 - acc: 0.3492\n",
      "Epoch 29/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.4105 - acc: 0.3511\n",
      "Epoch 30/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.4028 - acc: 0.3520\n",
      "Epoch 31/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.3930 - acc: 0.3535\n",
      "Epoch 32/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.3829 - acc: 0.3549\n",
      "Epoch 33/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3741 - acc: 0.3567\n",
      "Epoch 34/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3705 - acc: 0.3569\n",
      "Epoch 35/100\n",
      "61976/61976 [==============================] - 13s 212us/step - loss: 3.3674 - acc: 0.3575\n",
      "Epoch 36/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.3516 - acc: 0.3589\n",
      "Epoch 37/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3422 - acc: 0.3609\n",
      "Epoch 38/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3366 - acc: 0.3610\n",
      "Epoch 39/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.3263 - acc: 0.3618\n",
      "Epoch 40/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.3232 - acc: 0.3636\n",
      "Epoch 41/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.3110 - acc: 0.3647\n",
      "Epoch 42/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.3067 - acc: 0.3649\n",
      "Epoch 43/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2982 - acc: 0.3659\n",
      "Epoch 44/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2923 - acc: 0.3672\n",
      "Epoch 45/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2898 - acc: 0.3678\n",
      "Epoch 46/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2819 - acc: 0.3696\n",
      "Epoch 47/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2814 - acc: 0.3711\n",
      "Epoch 48/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2806 - acc: 0.3720\n",
      "Epoch 49/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.2746 - acc: 0.3714\n",
      "Epoch 50/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.2675 - acc: 0.3722\n",
      "Epoch 51/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2674 - acc: 0.3726\n",
      "Epoch 52/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2611 - acc: 0.3730\n",
      "Epoch 53/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2531 - acc: 0.3746\n",
      "Epoch 54/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.2521 - acc: 0.3757\n",
      "Epoch 55/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.2457 - acc: 0.3763\n",
      "Epoch 56/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.2397 - acc: 0.3763\n",
      "Epoch 57/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2336 - acc: 0.3774\n",
      "Epoch 58/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2326 - acc: 0.3780\n",
      "Epoch 59/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2253 - acc: 0.3797\n",
      "Epoch 60/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2197 - acc: 0.3799\n",
      "Epoch 61/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2116 - acc: 0.3807\n",
      "Epoch 62/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2075 - acc: 0.3818\n",
      "Epoch 63/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.2062 - acc: 0.3832\n",
      "Epoch 64/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.2028 - acc: 0.3816\n",
      "Epoch 65/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1995 - acc: 0.3822\n",
      "Epoch 66/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.1949 - acc: 0.3835\n",
      "Epoch 67/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1904 - acc: 0.3836\n",
      "Epoch 68/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1891 - acc: 0.3856\n",
      "Epoch 69/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1808 - acc: 0.3866\n",
      "Epoch 70/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1779 - acc: 0.3857\n",
      "Epoch 71/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1716 - acc: 0.3875\n",
      "Epoch 72/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1683 - acc: 0.3876\n",
      "Epoch 73/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1660 - acc: 0.3878\n",
      "Epoch 74/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1626 - acc: 0.3897\n",
      "Epoch 75/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.1570 - acc: 0.3903\n",
      "Epoch 76/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.1557 - acc: 0.3917\n",
      "Epoch 77/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1534 - acc: 0.3918\n",
      "Epoch 78/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1506 - acc: 0.3916\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1456 - acc: 0.3923\n",
      "Epoch 80/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1393 - acc: 0.3923\n",
      "Epoch 81/100\n",
      "61976/61976 [==============================] - 13s 210us/step - loss: 3.1382 - acc: 0.3930\n",
      "Epoch 82/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1367 - acc: 0.3942\n",
      "Epoch 83/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1333 - acc: 0.3947\n",
      "Epoch 84/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1251 - acc: 0.3940\n",
      "Epoch 85/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1240 - acc: 0.3963\n",
      "Epoch 86/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1194 - acc: 0.3956\n",
      "Epoch 87/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1168 - acc: 0.3966\n",
      "Epoch 88/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1133 - acc: 0.3970\n",
      "Epoch 89/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1103 - acc: 0.3963\n",
      "Epoch 90/100\n",
      "61976/61976 [==============================] - 13s 209us/step - loss: 3.1043 - acc: 0.3972\n",
      "Epoch 91/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.1059 - acc: 0.3976\n",
      "Epoch 92/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.1007 - acc: 0.3990\n",
      "Epoch 93/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0974 - acc: 0.3981\n",
      "Epoch 94/100\n",
      "61976/61976 [==============================] - 13s 211us/step - loss: 3.0929 - acc: 0.3996\n",
      "Epoch 95/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.0878 - acc: 0.4004\n",
      "Epoch 96/100\n",
      "61976/61976 [==============================] - 13s 207us/step - loss: 3.0853 - acc: 0.4003\n",
      "Epoch 97/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0840 - acc: 0.4004\n",
      "Epoch 98/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0798 - acc: 0.4023\n",
      "Epoch 99/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0758 - acc: 0.4015\n",
      "Epoch 100/100\n",
      "61976/61976 [==============================] - 13s 208us/step - loss: 3.0723 - acc: 0.4017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('complaints_pipe', Pipeline(memory=None,\n",
       "     steps=[('complaint_selector', ItemSelector(key='Жалобы (unigramm)')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, e...:48_2018/{epoch:02d}_{acc:.2f}.h5',\n",
       "    tb_log_dir='simple_models/log/nn14', validation_split=0.0))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = utils.load_data('data/test_data_complaints_repeats_doctors.csv')\n",
    "test_topics = np.load('data/topics_test_ngramm.npy')\n",
    "test = join_topics(test, test_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'Id_Записи': test['Id_Записи'], 'Код_диагноза': test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M42.1      2346\n",
       "J06.9      1734\n",
       "N76.0      1504\n",
       "Z32.1      1201\n",
       "N77.1*     1022\n",
       "N41.1       950\n",
       "J00         778\n",
       "M51         735\n",
       "I11         722\n",
       "K29.9       584\n",
       "M65         575\n",
       "K30         523\n",
       "J35.0       520\n",
       "Z01.8       505\n",
       "Z00.0       498\n",
       "M54.8       480\n",
       "N60.1       427\n",
       "M53.8       403\n",
       "D23.9       398\n",
       "H52.1       378\n",
       "Z01.4       355\n",
       "M54.2       354\n",
       "J01.0       330\n",
       "K29.5       317\n",
       "L30.8       289\n",
       "D25         264\n",
       "I83.9       261\n",
       "E04.2       245\n",
       "G90         225\n",
       "J04.1       223\n",
       "           ... \n",
       "H81.2         2\n",
       "Z10           2\n",
       "J02.9         2\n",
       "M54.1         2\n",
       "Z00.1         2\n",
       "E22.1         2\n",
       "L91.0         2\n",
       "H00.1         1\n",
       "I87.2         1\n",
       "M43.9         1\n",
       "J35.3         1\n",
       "H40.1         1\n",
       "L08.8         1\n",
       "L01.0         1\n",
       "S91.3         1\n",
       "N94.9         1\n",
       "M17.0         1\n",
       "G43.0         1\n",
       "I49           1\n",
       "B00.1         1\n",
       "M65-M68       1\n",
       "K58.9         1\n",
       "H65.1         1\n",
       "L01           1\n",
       "E05.0         1\n",
       "N84.1         1\n",
       "I86.1         1\n",
       "L04.0         1\n",
       "G44.1         1\n",
       "K81           1\n",
       "Name: Код_диагноза, Length: 307, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['Код_диагноза'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit/bow_simple_nn33_3dense_diag_all_compl_uni-n_gram_doctor_topics_clinic_repeats.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
